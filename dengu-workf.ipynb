{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T03:18:40.311568Z","iopub.status.busy":"2024-10-13T03:18:40.310732Z","iopub.status.idle":"2024-10-13T03:18:40.358030Z","shell.execute_reply":"2024-10-13T03:18:40.357002Z","shell.execute_reply.started":"2024-10-13T03:18:40.311486Z"},"trusted":true},"outputs":[{"ename":"UnicodeDecodeError","evalue":"'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDengue_final_as_MPOX.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m data\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n","File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n","File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n","\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte"]}],"source":["# Load the dataset\n","import pandas as pd\n","\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'])\n","data.set_index('Date', inplace=True)\n","\n","# Display the first few rows and the data types\n","data.head(), data.dtypes, data.info(), data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T03:18:57.821450Z","iopub.status.busy":"2024-10-13T03:18:57.820354Z","iopub.status.idle":"2024-10-13T03:18:57.845971Z","shell.execute_reply":"2024-10-13T03:18:57.844725Z","shell.execute_reply.started":"2024-10-13T03:18:57.821393Z"},"trusted":true},"outputs":[],"source":["# Adding lag features for New_Cases and New_Deaths\n","data['lag_1_cases'] = data['New_Cases'].shift(1)\n","data['lag_1_deaths'] = data['New_Deaths'].shift(1)\n","\n","# Dropping any rows with NaN values created by lag\n","data.dropna(inplace=True)\n","\n","# Adding additional time-based features\n","data['Year'] = data.index.year\n","data['Month'] = data.index.month\n","data['Day'] = data.index.day\n","data['DayOfWeek'] = data.index.dayofweek\n","data['WeekOfYear'] = data.index.isocalendar().week.astype(int)\n","\n","# Display the preprocessed dataset and the newly added features\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T03:19:07.075732Z","iopub.status.busy":"2024-10-13T03:19:07.074972Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","from statsmodels.graphics.tsaplots import plot_acf\n","\n","# Plotting correlations between features\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n","plt.title('Correlation Matrix')\n","plt.show()\n","\n","# Plotting the autocorrelation of New_Cases\n","plt.figure(figsize=(10, 4))\n","plot_acf(data['New_Cases'], lags=50)\n","plt.title('Autocorrelation of New Cases')\n","plt.xlabel('Lags')\n","plt.ylabel('Autocorrelation')\n","plt.show()\n","\n","# Plotting the autocorrelation of New_Deaths\n","plt.figure(figsize=(10, 4))\n","plot_acf(data['New_Deaths'], lags=50)\n","plt.title('Autocorrelation of New Deaths')\n","plt.xlabel('Lags')\n","plt.ylabel('Autocorrelation')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T03:19:21.672915Z","iopub.status.busy":"2024-10-13T03:19:21.672478Z","iopub.status.idle":"2024-10-13T03:19:22.243622Z","shell.execute_reply":"2024-10-13T03:19:22.242543Z","shell.execute_reply.started":"2024-10-13T03:19:21.672872Z"},"trusted":true},"outputs":[],"source":["from statsmodels.graphics.tsaplots import plot_pacf\n","\n","# Plotting the partial autocorrelation of New_Cases\n","plt.figure(figsize=(10, 4))\n","plot_pacf(data['New_Cases'], lags=50, method='ywm')\n","plt.title('Partial Autocorrelation of New Cases')\n","plt.xlabel('Lags')\n","plt.ylabel('Partial Autocorrelation')\n","plt.show()\n","\n","# Plotting the partial autocorrelation of New_Deaths\n","plt.figure(figsize=(10, 4))\n","plot_pacf(data['New_Deaths'], lags=50, method='ywm')\n","plt.title('Partial Autocorrelation of New Deaths')\n","plt.xlabel('Lags')\n","plt.ylabel('Partial Autocorrelation')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T03:17:08.123841Z","iopub.status.busy":"2024-10-13T03:17:08.123038Z","iopub.status.idle":"2024-10-13T03:17:08.182039Z","shell.execute_reply":"2024-10-13T03:17:08.180950Z","shell.execute_reply.started":"2024-10-13T03:17:08.123795Z"},"trusted":true},"outputs":[],"source":["# Load the dataset\n","import pandas as pd\n","import numpy as np\n","\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'])\n","data.set_index('Date', inplace=True)\n","\n","# Generate lag features for New_Cases and New_Deaths\n","data['lag_1_cases'] = data['New_Cases'].shift(1)\n","data['lag_2_cases'] = data['New_Cases'].shift(2)\n","data['lag_7_cases'] = data['New_Cases'].shift(7)\n","\n","data['lag_1_deaths'] = data['New_Deaths'].shift(1)\n","data['lag_2_deaths'] = data['New_Deaths'].shift(2)\n","data['lag_7_deaths'] = data['New_Deaths'].shift(7)\n","\n","# Generate rolling statistics for New_Cases and New_Deaths\n","data['rolling_mean_7_cases'] = data['New_Cases'].rolling(window=7).mean()\n","data['rolling_std_7_cases'] = data['New_Cases'].rolling(window=7).std()\n","\n","data['rolling_mean_7_deaths'] = data['New_Deaths'].rolling(window=7).mean()\n","data['rolling_std_7_deaths'] = data['New_Deaths'].rolling(window=7).std()\n","\n","# Dropping any rows with NaN values created by lag/rolling features\n","data.dropna(inplace=True)\n","\n","# Adding additional time-based features\n","data['Year'] = data.index.year\n","data['Month'] = data.index.month\n","data['Day'] = data.index.day\n","data['DayOfWeek'] = data.index.dayofweek\n","data['WeekOfYear'] = data.index.isocalendar().week.astype(int)\n","\n","# Display the preprocessed dataset and the newly added features\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T03:17:23.438780Z","iopub.status.busy":"2024-10-13T03:17:23.438386Z","iopub.status.idle":"2024-10-13T03:17:43.504292Z","shell.execute_reply":"2024-10-13T03:17:43.503111Z","shell.execute_reply.started":"2024-10-13T03:17:23.438741Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from xgboost import XGBRegressor\n","import numpy as np\n","\n","# Function to evaluate the model's performance\n","def evaluate_model(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","    return mse, rmse, mae, r2\n","\n","# Splitting the dataset for New_Cases\n","X_cases = data.drop(['New_Cases', 'New_Deaths'], axis=1)\n","y_cases = data['New_Cases']\n","X_train_cases, X_test_cases, y_train_cases, y_test_cases = train_test_split(X_cases, y_cases, test_size=0.2, random_state=42)\n","\n","# Splitting the dataset for New_Deaths\n","y_deaths = data['New_Deaths']\n","X_train_deaths, X_test_deaths, y_train_deaths, y_test_deaths = train_test_split(X_cases, y_deaths, test_size=0.2, random_state=42)\n","\n","# Define models\n","xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=1000, max_depth=5, learning_rate=0.2, random_state=42)\n","rf_model = RandomForestRegressor(n_estimators=1000, random_state=42)\n","gb_model = GradientBoostingRegressor(n_estimators=1000, random_state=42)\n","\n","# Train the models on New Cases\n","models = [xgb_model, rf_model, gb_model]\n","predictions_cases = []\n","\n","for model in models:\n","    model.fit(X_train_cases, y_train_cases)\n","    preds = model.predict(X_test_cases)\n","    predictions_cases.append(preds)\n","\n","# Averaging predictions for New Cases\n","final_predictions_cases = np.mean(predictions_cases, axis=0)\n","\n","# Evaluate for New Cases\n","mse_cases, rmse_cases, mae_cases, r2_cases = evaluate_model(y_test_cases, final_predictions_cases)\n","\n","# Train the models on New Deaths\n","predictions_deaths = []\n","\n","for model in models:\n","    model.fit(X_train_deaths, y_train_deaths)\n","    preds = model.predict(X_test_deaths)\n","    predictions_deaths.append(preds)\n","\n","# Averaging predictions for New Deaths\n","final_predictions_deaths = np.mean(predictions_deaths, axis=0)\n","\n","# Evaluate for New Deaths\n","mse_deaths, rmse_deaths, mae_deaths, r2_deaths = evaluate_model(y_test_deaths, final_predictions_deaths)\n","\n","# Print the evaluation metrics for New Cases\n","print(f\"Evaluation Metrics for New Cases:\")\n","print(f\"RMSE: {rmse_cases}\")\n","print(f\"MSE: {mse_cases}\")\n","print(f\"MAE: {mae_cases}\")\n","print(f\"R-squared: {r2_cases}\\n\")\n","\n","# Print the evaluation metrics for New Deaths\n","print(f\"Evaluation Metrics for New Deaths:\")\n","print(f\"RMSE: {rmse_deaths}\")\n","print(f\"MSE: {mse_deaths}\")\n","print(f\"MAE: {mae_deaths}\")\n","print(f\"R-squared: {r2_deaths}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T03:25:58.644513Z","iopub.status.busy":"2024-10-13T03:25:58.644114Z","iopub.status.idle":"2024-10-13T03:25:58.678418Z","shell.execute_reply":"2024-10-13T03:25:58.677299Z","shell.execute_reply.started":"2024-10-13T03:25:58.644474Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","# Convert the 'Date' column to datetime format and handle any potential errors\n","data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n","\n","# Drop any rows where the 'Date' conversion failed\n","data = data.dropna(subset=['Date'])\n","\n","# Strip any extra spaces from column names\n","data.columns = data.columns.str.strip()\n","\n","# Set the 'Date' column as the index\n","data.set_index('Date', inplace=True)\n","\n","# Define the last known values for cases and deaths\n","last_known_cases = data.iloc[-1]['New_Cases']\n","last_known_deaths = data.iloc[-1]['New_Deaths']\n","last_total_cases = data.iloc[-1]['Total_Cases']\n","last_total_deaths = data.iloc[-1]['Total_Deaths']\n","\n","# Generate a date range for the next 365 days starting from the day after the last date in the dataset\n","future_dates = pd.date_range(start=data.index[-1] + pd.Timedelta(days=1), periods=365, freq='D')\n","\n","# Create a DataFrame for these dates with columns ordered as per the trained model\n","future_data = pd.DataFrame({\n","    'Total_Cases': last_total_cases,\n","    'Total_Deaths': last_total_deaths,\n","    'lag_1_cases': last_known_cases,\n","    'lag_2_cases': last_known_cases,  # Using last known as lag_2 since we don't have it for future dates\n","    'lag_7_cases': last_known_cases,  # Same logic for lag_7\n","    'lag_1_deaths': last_known_deaths,\n","    'lag_2_deaths': last_known_deaths,\n","    'lag_7_deaths': last_known_deaths,\n","    'Year': future_dates.year,\n","    'Month': future_dates.month,\n","    'Day': future_dates.day,\n","    'DayOfWeek': future_dates.dayofweek.astype(int),\n","    'WeekOfYear': future_dates.isocalendar().week.astype(int)\n","}, index=future_dates)\n","\n","# Add rolling mean and std for future dates (can be assumed constant or similar to the last known values)\n","future_data['rolling_mean_7_cases'] = data['New_Cases'].rolling(7).mean().iloc[-1]\n","future_data['rolling_std_7_cases'] = data['New_Cases'].rolling(7).std().iloc[-1]\n","\n","future_data['rolling_mean_7_deaths'] = data['New_Deaths'].rolling(7).mean().iloc[-1]\n","future_data['rolling_std_7_deaths'] = data['New_Deaths'].rolling(7).std().iloc[-1]\n","\n","# Display the first few rows of future_data\n","print(future_data.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T03:26:23.645825Z","iopub.status.busy":"2024-10-13T03:26:23.645397Z","iopub.status.idle":"2024-10-13T03:26:23.656898Z","shell.execute_reply":"2024-10-13T03:26:23.655738Z","shell.execute_reply.started":"2024-10-13T03:26:23.645784Z"},"trusted":true},"outputs":[],"source":["# Load and display the predictions\n","predicted_cases = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","print(predicted_cases.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T03:27:19.572935Z","iopub.status.busy":"2024-10-13T03:27:19.572474Z","iopub.status.idle":"2024-10-13T03:43:15.105219Z","shell.execute_reply":"2024-10-13T03:43:15.104048Z","shell.execute_reply.started":"2024-10-13T03:27:19.572891Z"},"trusted":true},"outputs":[],"source":["import warnings\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from skopt import BayesSearchCV\n","\n","# Suppress specific LightGBM warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# Custom logger to suppress LightGBM output\n","class SilentLogger:\n","    def info(self, msg):\n","        pass\n","\n","    def warning(self, msg):\n","        pass\n","\n","import lightgbm as lgb\n","lgb.register_logger(SilentLogger())\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'])\n","data.set_index('Date', inplace=True)\n","\n","# Generate lag features for New_Cases and New_Deaths\n","for lag in [1, 2, 7, 14, 30]:\n","    data[f'lag_{lag}_cases'] = data['New_Cases'].shift(lag)\n","    data[f'lag_{lag}_deaths'] = data['New_Deaths'].shift(lag)\n","\n","# Generate rolling statistics for New_Cases and New_Deaths\n","for window in [7, 14, 30]:\n","    data[f'rolling_mean_{window}_cases'] = data['New_Cases'].rolling(window=window).mean()\n","    data[f'rolling_std_{window}_cases'] = data['New_Cases'].rolling(window=window).std()\n","    data[f'rolling_mean_{window}_deaths'] = data['New_Deaths'].rolling(window=window).mean()\n","    data[f'rolling_std_{window}_deaths'] = data['New_Deaths'].rolling(window=window).std()\n","\n","# Add interaction features\n","data['cases_deaths_interaction'] = data['New_Cases'] * data['New_Deaths']\n","\n","# Dropping any rows with NaN values created by lag/rolling features\n","data.dropna(inplace=True)\n","\n","# Adding additional time-based features\n","data['Year'] = data.index.year\n","data['Month'] = data.index.month\n","data['Day'] = data.index.day\n","\n","# Log transform the target variables\n","data['log_New_Cases'] = np.log1p(data['New_Cases'])\n","data['log_New_Deaths'] = np.log1p(data['New_Deaths'])\n","\n","# Standardize the features\n","features = data.drop(['New_Cases', 'New_Deaths', 'log_New_Cases', 'log_New_Deaths'], axis=1)\n","scaler = StandardScaler()\n","scaled_features = scaler.fit_transform(features)\n","data[features.columns] = scaled_features\n","\n","# Feature importance via XGBoost to keep the most relevant features\n","xgb_temp = XGBRegressor(objective='reg:squarederror', random_state=42)\n","xgb_temp.fit(scaled_features, data['log_New_Cases'])\n","importance = xgb_temp.feature_importances_\n","important_features = features.columns[np.argsort(importance)[-20:]]  # Select top 20 features\n","\n","# Reduce dimensionality using PCA\n","pca = PCA(n_components=10)  # Reducing dimensions to top 10 principal components\n","X_pca = pca.fit_transform(data[important_features])\n","\n","# Update the dataset with PCA features\n","data_pca = pd.DataFrame(X_pca, index=data.index, columns=[f'PC{i}' for i in range(1, 11)])\n","\n","# Splitting the dataset for New_Cases\n","X_cases = data_pca\n","y_cases = data['log_New_Cases']\n","X_train_cases, X_test_cases, y_train_cases, y_test_cases = train_test_split(X_cases, y_cases, test_size=0.2, random_state=42)\n","\n","# Splitting the dataset for New_Deaths\n","y_deaths = data['log_New_Deaths']\n","X_train_deaths, X_test_deaths, y_train_deaths, y_test_deaths = train_test_split(X_cases, y_deaths, test_size=0.2, random_state=42)\n","\n","# Define base models including LightGBM\n","xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n","rf_model = RandomForestRegressor(random_state=42)\n","gb_model = GradientBoostingRegressor(random_state=42)\n","lgbm_model = LGBMRegressor(random_state=42)\n","\n","# Fit all the models to the training data\n","xgb_model.fit(X_train_cases, y_train_cases)\n","rf_model.fit(X_train_cases, y_train_cases)\n","gb_model.fit(X_train_cases, y_train_cases)\n","lgbm_model.fit(X_train_cases, y_train_cases)\n","\n","# Hyperparameter tuning for LightGBM using Bayesian Optimization\n","param_grid_lgbm = {\n","    'n_estimators': [100, 500, 1000],\n","    'max_depth': [3, 5, 7],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'subsample': [0.8, 1.0],\n","    'colsample_bytree': [0.8, 1.0],\n","    'num_leaves': [31, 64, 128]\n","}\n","\n","bayes_search_lgbm = BayesSearchCV(estimator=lgbm_model, search_spaces=param_grid_lgbm, cv=20, n_iter=100, scoring='neg_mean_squared_error', random_state=42, verbose=1)\n","bayes_search_lgbm.fit(X_train_cases, y_train_cases)\n","\n","# Best LightGBM model\n","best_lgbm_model = bayes_search_lgbm.best_estimator_\n","\n","# Stacking Regressor with the best models\n","estimators = [\n","    ('xgb', xgb_model),\n","    ('rf', rf_model),\n","    ('gb', gb_model),\n","    ('lgbm', best_lgbm_model)\n","]\n","stacking_regressor_cases = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor(random_state=42))\n","stacking_regressor_cases.fit(X_train_cases, y_train_cases)\n","\n","stacking_regressor_deaths = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor(random_state=42))\n","stacking_regressor_deaths.fit(X_train_deaths, y_train_deaths)\n","\n","# Function to evaluate and print model performance\n","def evaluate_models(models, X_test, y_test, y_test_orig, target_name):\n","    metrics = {}\n","    for name, model in models.items():\n","        y_pred = model.predict(X_test)\n","        y_pred = np.expm1(y_pred)  # Inverse of log transformation\n","        mse = mean_squared_error(y_test_orig, y_pred)\n","        rmse = np.sqrt(mse)\n","        mae = mean_absolute_error(y_test_orig, y_pred)\n","        r2 = r2_score(y_test_orig, y_pred)\n","        metrics[name] = {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R-squared': r2}\n","        print(f\"Target: {target_name} | Model: {name}\")\n","        print(f\"RMSE: {rmse}\")\n","        print(f\"MSE: {mse}\")\n","        print(f\"MAE: {mae}\")\n","        print(f\"R-squared: {r2}\\n\")\n","    return metrics\n","\n","# Evaluate for New Cases\n","models_cases = {\n","    'XGBoost': xgb_model,\n","    'RandomForest': rf_model,\n","    'GradientBoosting': gb_model,\n","    'LightGBM': best_lgbm_model,\n","    'Stacking': stacking_regressor_cases\n","}\n","metrics_cases = evaluate_models(models_cases, X_test_cases, y_test_cases, np.expm1(y_test_cases), \"New Cases\")\n","\n","# Evaluate for New Deaths\n","models_deaths = {\n","    'XGBoost': xgb_model,\n","    'RandomForest': rf_model,\n","    'GradientBoosting': gb_model,\n","    'LightGBM': best_lgbm_model,\n","    'Stacking': stacking_regressor_deaths\n","}\n","metrics_deaths = evaluate_models(models_deaths, X_test_deaths, y_test_deaths, np.expm1(y_test_deaths), \"New Deaths\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T06:17:30.558743Z","iopub.status.busy":"2024-10-13T06:17:30.558377Z","iopub.status.idle":"2024-10-13T06:36:08.940958Z","shell.execute_reply":"2024-10-13T06:36:08.939831Z","shell.execute_reply.started":"2024-10-13T06:17:30.558704Z"},"trusted":true},"outputs":[],"source":["import warnings\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from sklearn.decomposition import PCA\n","from skopt import BayesSearchCV\n","from tpot import TPOTRegressor\n","\n","# Suppress warnings\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# TensorFlow warning suppression\n","tf.get_logger().setLevel('ERROR')\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'])\n","data.set_index('Date', inplace=True)\n","\n","# ----------------- Feature Engineering -----------------\n","# Generate lag features for New_Cases and New_Deaths\n","for lag in [1, 2, 7, 14, 30]:\n","    data[f'lag_{lag}_cases'] = data['New_Cases'].shift(lag)\n","    data[f'lag_{lag}_deaths'] = data['New_Deaths'].shift(lag)\n","\n","# Generate rolling statistics for trends and moving averages\n","for window in [7, 14, 30]:\n","    data[f'rolling_mean_{window}_cases'] = data['New_Cases'].rolling(window=window).mean()\n","    data[f'rolling_std_{window}_cases'] = data['New_Cases'].rolling(window=window).std()\n","    data[f'rolling_mean_{window}_deaths'] = data['New_Deaths'].rolling(window=window).mean()\n","    data[f'rolling_std_{window}_deaths'] = data['New_Deaths'].rolling(window=window).std()\n","\n","# Add interaction features\n","data['cases_deaths_interaction'] = data['New_Cases'] * data['New_Deaths']\n","\n","# Adding seasonal indicator features (e.g., high season months)\n","data['Season'] = data.index.month.isin([6, 7, 8, 9, 10]).astype(int)\n","\n","# Dropping any rows with NaN values created by lag/rolling features\n","data.dropna(inplace=True)\n","\n","# Adding additional time-based features\n","data['Year'] = data.index.year\n","data['Month'] = data.index.month\n","data['Day'] = data.index.day\n","\n","# Log transform the target variables\n","data['log_New_Cases'] = np.log1p(data['New_Cases'])\n","data['log_New_Deaths'] = np.log1p(data['New_Deaths'])\n","\n","# Standardize the features\n","features = data.drop(['New_Cases', 'New_Deaths', 'log_New_Cases', 'log_New_Deaths'], axis=1)\n","scaler = StandardScaler()\n","scaled_features = scaler.fit_transform(features)\n","data[features.columns] = scaled_features\n","\n","# Feature importance via XGBoost to keep the most relevant features\n","xgb_temp = XGBRegressor(objective='reg:squarederror', random_state=42)\n","xgb_temp.fit(scaled_features, data['log_New_Cases'])\n","importance = xgb_temp.feature_importances_\n","important_features = features.columns[np.argsort(importance)[-20:]]  # Select top 20 features\n","\n","# Reduce dimensionality using PCA\n","pca = PCA(n_components=10)  # Reducing dimensions to top 10 principal components\n","X_pca = pca.fit_transform(data[important_features])\n","\n","# ----------------- LSTM Model for Time Series -----------------\n","scaler_lstm = MinMaxScaler(feature_range=(0, 1))\n","\n","def prepare_lstm_data(data, n_steps):\n","    X, y = [], []\n","    for i in range(n_steps, len(data)):\n","        X.append(data[i-n_steps:i, 0])\n","        y.append(data[i, 0])\n","    return np.array(X), np.array(y)\n","\n","# Train LSTM model\n","def train_lstm_model(data, n_steps=60, epochs=100, batch_size=32):\n","    data_scaled = scaler_lstm.fit_transform(data.reshape(-1, 1))\n","    X, y = prepare_lstm_data(data_scaled, n_steps)\n","    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n","\n","    model = Sequential()\n","    model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))\n","    model.add(Dropout(0.2))\n","    model.add(LSTM(units=50, return_sequences=False))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(units=1))\n","\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","    model.fit(X, y, epochs=epochs, batch_size=batch_size, verbose=0)\n","    return model, scaler_lstm\n","\n","# Prepare data for LSTM\n","new_cases_data = data['New_Cases'].values\n","new_deaths_data = data['New_Deaths'].values\n","\n","# Train LSTM models for both target variables\n","lstm_model_cases, scaler_cases = train_lstm_model(new_cases_data)\n","lstm_model_deaths, scaler_deaths = train_lstm_model(new_deaths_data)\n","\n","# Function to predict with LSTM\n","def predict_lstm(model, data, scaler, n_steps=60):\n","    data_scaled = scaler.transform(data.reshape(-1, 1))\n","    X_test = []\n","    for i in range(n_steps, len(data_scaled)):\n","        X_test.append(data_scaled[i-n_steps:i, 0])\n","    X_test = np.array(X_test)\n","    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n","    predictions = model.predict(X_test)\n","    return scaler.inverse_transform(predictions)\n","\n","# Make LSTM predictions\n","lstm_predictions_cases = predict_lstm(lstm_model_cases, new_cases_data, scaler_cases)\n","lstm_predictions_deaths = predict_lstm(lstm_model_deaths, new_deaths_data, scaler_deaths)\n","\n","# ----------------- AutoML with TPOT -----------------\n","X_cases = data_pca\n","y_cases = data['log_New_Cases']\n","X_train_cases, X_test_cases, y_train_cases, y_test_cases = train_test_split(X_cases, y_cases, test_size=0.2, random_state=42)\n","\n","# Use TPOT to automatically optimize and select the best model\n","tpot = TPOTRegressor(verbosity=2, generations=10, population_size=20, random_state=42)\n","tpot.fit(X_train_cases, y_train_cases)\n","tpot_predictions = np.expm1(tpot.predict(X_test_cases))\n","\n","# Evaluate TPOT model predictions\n","mse_tpot = mean_squared_error(np.expm1(y_test_cases), tpot_predictions)\n","rmse_tpot = np.sqrt(mse_tpot)\n","mae_tpot = mean_absolute_error(np.expm1(y_test_cases), tpot_predictions)\n","r2_tpot = r2_score(np.expm1(y_test_cases), tpot_predictions)\n","\n","print(f\"TPOT Model - New Cases:\")\n","print(f\"RMSE: {rmse_tpot}\")\n","print(f\"MSE: {mse_tpot}\")\n","print(f\"MAE: {mae_tpot}\")\n","print(f\"R-squared: {r2_tpot}\")\n","\n","# ----------------- Model Evaluation -----------------\n","# Evaluate the LSTM model predictions for New Cases\n","mse_cases_lstm = mean_squared_error(new_cases_data[-len(lstm_predictions_cases):], lstm_predictions_cases)\n","rmse_cases_lstm = np.sqrt(mse_cases_lstm)\n","mae_cases_lstm = mean_absolute_error(new_cases_data[-len(lstm_predictions_cases):], lstm_predictions_cases)\n","r2_cases_lstm = r2_score(new_cases_data[-len(lstm_predictions_cases):], lstm_predictions_cases)\n","\n","print(f\"LSTM Model - New Cases:\")\n","print(f\"RMSE: {rmse_cases_lstm}\")\n","print(f\"MSE: {mse_cases_lstm}\")\n","print(f\"MAE: {mae_cases_lstm}\")\n","print(f\"R-squared: {r2_cases_lstm}\")\n","\n","# Evaluate the LSTM model predictions for New Deaths\n","mse_deaths_lstm = mean_squared_error(new_deaths_data[-len(lstm_predictions_deaths):], lstm_predictions_deaths)\n","rmse_deaths_lstm = np.sqrt(mse_deaths_lstm)\n","mae_deaths_lstm = mean_absolute_error(new_deaths_data[-len(lstm_predictions_deaths):], lstm_predictions_deaths)\n","r2_deaths_lstm = r2_score(new_deaths_data[-len(lstm_predictions_deaths):], lstm_predictions_deaths)\n","\n","print(f\"LSTM Model - New Deaths:\")\n","print(f\"RMSE: {rmse_deaths_lstm}\")\n","print(f\"MSE: {mse_deaths_lstm}\")\n","print(f\"MAE: {mae_deaths_lstm}\")\n","print(f\"R-squared: {r2_deaths_lstm}\")"]},{"cell_type":"markdown","metadata":{},"source":["**end here**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T06:47:21.625892Z","iopub.status.busy":"2024-10-13T06:47:21.625033Z","iopub.status.idle":"2024-10-13T06:58:10.105435Z","shell.execute_reply":"2024-10-13T06:58:10.104158Z","shell.execute_reply.started":"2024-10-13T06:47:21.625845Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from xgboost import XGBRegressor\n","\n","# Define the parameter grid\n","param_grid = {\n","    'n_estimators': [100, 500, 1000],\n","    'max_depth': [3, 5, 7],\n","    'learning_rate': [0.01],\n","    'subsample': [0.8, 1.0],\n","    'colsample_bytree': [0.8, 1.0],\n","    'min_child_weight': [3, 5],\n","    'reg_alpha': [0.1, 0.5],\n","    'reg_lambda': [1, 1.5]\n","}\n","\n","# Initialize the model\n","xgb_model = XGBRegressor(objective='reg:squarederror')\n","\n","# Perform grid search\n","grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=2)\n","grid_search.fit(X_train_cases, y_train_cases)\n","\n","# Get the best parameters and the best score\n","best_params = grid_search.best_params_\n","best_score = -grid_search.best_score_\n","\n","print(f\"Best Parameters: {best_params}\")\n","print(f\"Best RMSE Score: {np.sqrt(best_score)}\")"]},{"cell_type":"markdown","metadata":{},"source":["**RANDOM REGRESSOR**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T06:59:08.864984Z","iopub.status.busy":"2024-10-13T06:59:08.864543Z","iopub.status.idle":"2024-10-13T06:59:08.892701Z","shell.execute_reply":"2024-10-13T06:59:08.891466Z","shell.execute_reply.started":"2024-10-13T06:59:08.864942Z"},"trusted":true},"outputs":[],"source":["# Load the dataset\n","import pandas as pd\n","\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'])\n","data.set_index('Date', inplace=True)\n","\n","print(data.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T06:59:24.345628Z","iopub.status.busy":"2024-10-13T06:59:24.345202Z","iopub.status.idle":"2024-10-13T06:59:24.365176Z","shell.execute_reply":"2024-10-13T06:59:24.363981Z","shell.execute_reply.started":"2024-10-13T06:59:24.345585Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# Load the dataset\n","dengue_data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","# Convert the 'Date' column to datetime format\n","dengue_data['Date'] = pd.to_datetime(dengue_data['Date'], format='%m/%d/%Y')\n","\n","# Set the 'Date' column as the index\n","dengue_data.set_index('Date', inplace=True)\n","\n","# Extract month and day of the week from the index\n","dengue_data['Month'] = dengue_data.index.month\n","dengue_data['DayOfWeek'] = dengue_data.index.dayofweek\n","\n","# Prepare the features for modeling\n","features = dengue_data[['Month', 'DayOfWeek']]  # Including the new features\n","target = dengue_data['Total_Deaths']  # Corrected column name\n","\n","\n","# Print the first few rows to confirm the changes\n","print(features.head())\n","print(target.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T06:59:28.744872Z","iopub.status.busy":"2024-10-13T06:59:28.744448Z","iopub.status.idle":"2024-10-13T07:01:58.984489Z","shell.execute_reply":"2024-10-13T07:01:58.983359Z","shell.execute_reply.started":"2024-10-13T06:59:28.744830Z"},"trusted":true},"outputs":[],"source":["# Import necessary libraries for modeling and validation\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n","import numpy as np\n","\n","# Initialize the model with updated parameters\n","rf_model = RandomForestRegressor(n_estimators=1000, max_depth=10, random_state=42)\n","\n","# Cross-validation using time series split\n","tscv = TimeSeriesSplit(n_splits=100)\n","cv_scores = cross_val_score(rf_model, features, target, cv=tscv, scoring='neg_mean_squared_error')\n","\n","# Calculate RMSE for each fold\n","cv_rmse_scores = np.sqrt(-cv_scores)\n","print('Cross-validated RMSE scores:', cv_rmse_scores)\n","print('Average RMSE:', np.mean(cv_rmse_scores))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:02:44.903746Z","iopub.status.busy":"2024-10-13T07:02:44.903309Z","iopub.status.idle":"2024-10-13T07:02:44.910213Z","shell.execute_reply":"2024-10-13T07:02:44.908986Z","shell.execute_reply.started":"2024-10-13T07:02:44.903703Z"},"trusted":true},"outputs":[],"source":["# Split the data into training and testing sets for New Cases and New Deaths\n","train_cases = data['New_Cases'][:-30]\n","test_cases = data['New_Cases'][-30:]\n","\n","train_deaths = data['New_Deaths'][:-30]\n","test_deaths = data['New_Deaths'][-30:]\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:02:48.424828Z","iopub.status.busy":"2024-10-13T07:02:48.423904Z","iopub.status.idle":"2024-10-13T07:02:48.441814Z","shell.execute_reply":"2024-10-13T07:02:48.440801Z","shell.execute_reply.started":"2024-10-13T07:02:48.424782Z"},"trusted":true},"outputs":[],"source":["# Load the dataset\n","import pandas as pd\n","\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'])\n","data.set_index('Date', inplace=True)\n","\n","print(data.head())\n","            "]},{"cell_type":"markdown","metadata":{},"source":["**time SERIS**"]},{"cell_type":"markdown","metadata":{},"source":["**NEW START AGAIN**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:02:51.584510Z","iopub.status.busy":"2024-10-13T07:02:51.583753Z","iopub.status.idle":"2024-10-13T07:03:04.220392Z","shell.execute_reply":"2024-10-13T07:03:04.219134Z","shell.execute_reply.started":"2024-10-13T07:02:51.584465Z"},"trusted":true},"outputs":[],"source":["!pip install pmdarima"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:03:44.512100Z","iopub.status.busy":"2024-10-13T07:03:44.511649Z","iopub.status.idle":"2024-10-13T07:03:44.536792Z","shell.execute_reply":"2024-10-13T07:03:44.535704Z","shell.execute_reply.started":"2024-10-13T07:03:44.512041Z"},"trusted":true},"outputs":[],"source":["# Load the dataset\n","import pandas as pd\n","\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'])\n","data.set_index('Date', inplace=True)\n","\n","print(data.head())\n","            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:07:02.507114Z","iopub.status.busy":"2024-10-13T07:07:02.506642Z","iopub.status.idle":"2024-10-13T07:07:05.029029Z","shell.execute_reply":"2024-10-13T07:07:05.027906Z","shell.execute_reply.started":"2024-10-13T07:07:02.507046Z"},"trusted":true},"outputs":[],"source":["##just ei part###\n","\n","import pandas as pd\n","from prophet import Prophet\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","# Convert the 'Date' column to datetime\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","\n","# Apply log transformation to avoid negative predictions\n","data['Log_New_Cases'] = np.log1p(data['New_Cases'])  # log1p handles zeros safely\n","data['Log_New_Deaths'] = np.log1p(data['New_Deaths'])\n","\n","# Prepare data for Prophet (New Cases)\n","df_new_cases = data[['Date', 'Log_New_Cases']].rename(columns={'Date': 'ds', 'Log_New_Cases': 'y'})\n","\n","# Prepare data for Prophet (New Deaths)\n","df_new_deaths = data[['Date', 'Log_New_Deaths']].rename(columns={'Date': 'ds', 'Log_New_Deaths': 'y'})\n","\n","# Initialize Prophet model for New Cases with fine-tuned hyperparameters\n","model_cases = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.05,\n","    seasonality_prior_scale=2.0\n",")\n","model_cases.fit(df_new_cases)\n","\n","# Initialize Prophet model for New Deaths with fine-tuned hyperparameters\n","model_deaths = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.05,\n","    seasonality_prior_scale=2.0\n",")\n","model_deaths.fit(df_new_deaths)\n","\n","# Generate future dates for prediction\n","future_cases = model_cases.make_future_dataframe(periods=60)\n","future_deaths = model_deaths.make_future_dataframe(periods=60)\n","\n","# Forecasting\n","forecast_cases = model_cases.predict(future_cases)\n","forecast_deaths = model_deaths.predict(future_deaths)\n","\n","# Inverse log transformation to bring predictions back to original scale\n","forecast_cases['yhat'] = np.expm1(forecast_cases['yhat'])  # expm1 reverses log1p\n","forecast_deaths['yhat'] = np.expm1(forecast_deaths['yhat'])\n","\n","# Ensure no negative predictions by clipping at 0\n","forecast_cases['yhat'] = forecast_cases['yhat'].clip(lower=0)\n","forecast_deaths['yhat'] = forecast_deaths['yhat'].clip(lower=0)\n","\n","# Merge actual and predicted values for evaluation (New Cases)\n","df_merged_cases = pd.merge(data[['Date', 'New_Cases']], forecast_cases[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_cases.dropna(inplace=True)\n","\n","# Merge actual and predicted values for evaluation (New Deaths)\n","df_merged_deaths = pd.merge(data[['Date', 'New_Deaths']], forecast_deaths[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_deaths.dropna(inplace=True)\n","\n","# Evaluation metrics for New Cases\n","rmse_cases = np.sqrt(mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat']))\n","mse_cases = mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","mae_cases = mean_absolute_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","r2_cases = r2_score(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","\n","# Evaluation metrics for New Deaths\n","rmse_deaths = np.sqrt(mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat']))\n","mse_deaths = mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","mae_deaths = mean_absolute_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","r2_deaths = r2_score(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","\n","# Print evaluation metrics\n","print(f\"New Cases - RMSE: {rmse_cases}, MSE: {mse_cases}, MAE: {mae_cases}, R-squared: {r2_cases}\")\n","print(f\"New Deaths - RMSE: {rmse_deaths}, MSE: {mse_deaths}, MAE: {mae_deaths}, R-squared: {r2_deaths}\")\n","\n","# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_cases['Date'], df_merged_cases['New_Cases'], label='Actual New Cases')\n","plt.plot(df_merged_cases['Date'], df_merged_cases['yhat'], label='Predicted New Cases')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (Scaled)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['New_Deaths'], label='Actual New Deaths')\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['yhat'], label='Predicted New Deaths')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (Scaled)')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:07:21.626325Z","iopub.status.busy":"2024-10-13T07:07:21.625460Z","iopub.status.idle":"2024-10-13T07:07:35.082665Z","shell.execute_reply":"2024-10-13T07:07:35.081669Z","shell.execute_reply.started":"2024-10-13T07:07:21.626277Z"},"trusted":true},"outputs":[],"source":["# Install required libraries\n","!pip install prophet scikit-learn matplotlib\n","\n","import pandas as pd\n","from prophet import Prophet\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","# Convert the 'Date' column to datetime\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","\n","# Apply log transformation to stabilize variance\n","data['Log_New_Cases'] = np.log1p(data['New_Cases'])\n","data['Log_New_Deaths'] = np.log1p(data['New_Deaths'])\n","\n","# Prepare data for Prophet (New Cases)\n","df_new_cases = data[['Date', 'Log_New_Cases']].rename(columns={'Date': 'ds', 'Log_New_Cases': 'y'})\n","\n","# Prepare data for Prophet (New Deaths)\n","df_new_deaths = data[['Date', 'Log_New_Deaths']].rename(columns={'Date': 'ds', 'Log_New_Deaths': 'y'})\n","\n","# Initialize Prophet model for New Cases\n","model_cases = Prophet(\n","    seasonality_mode='multiplicative', \n","    yearly_seasonality=True, \n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.1  # Controls flexibility, lower values less flexible\n",")\n","model_cases.fit(df_new_cases)\n","\n","# Initialize Prophet model for New Deaths\n","model_deaths = Prophet(\n","    seasonality_mode='multiplicative', \n","    yearly_seasonality=True, \n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.1\n",")\n","model_deaths.fit(df_new_deaths)\n","\n","# Generate future dates for prediction\n","future_cases = model_cases.make_future_dataframe(periods=60)\n","future_deaths = model_deaths.make_future_dataframe(periods=60)\n","\n","# Forecasting\n","forecast_cases = model_cases.predict(future_cases)\n","forecast_deaths = model_deaths.predict(future_deaths)\n","\n","# Inverse log transformation to bring the predictions back to the original scale\n","forecast_cases['yhat'] = np.expm1(forecast_cases['yhat'])\n","forecast_deaths['yhat'] = np.expm1(forecast_deaths['yhat'])\n","\n","# Merge actual and predicted values for evaluation (New Cases)\n","df_merged_cases = pd.merge(data[['Date', 'New_Cases']], forecast_cases[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_cases.dropna(inplace=True)\n","\n","# Merge actual and predicted values for evaluation (New Deaths)\n","df_merged_deaths = pd.merge(data[['Date', 'New_Deaths']], forecast_deaths[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_deaths.dropna(inplace=True)\n","\n","# Evaluation metrics for New Cases\n","rmse_cases = np.sqrt(mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat']))\n","mse_cases = mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","mae_cases = mean_absolute_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","r2_cases = r2_score(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","\n","# Evaluation metrics for New Deaths\n","rmse_deaths = np.sqrt(mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat']))\n","mse_deaths = mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","mae_deaths = mean_absolute_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","r2_deaths = r2_score(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","\n","# Print evaluation metrics\n","print(f\"New Cases - RMSE: {rmse_cases}, MSE: {mse_cases}, MAE: {mae_cases}, R-squared: {r2_cases}\")\n","print(f\"New Deaths - RMSE: {rmse_deaths}, MSE: {mse_deaths}, MAE: {mae_deaths}, R-squared: {r2_deaths}\")\n","\n","# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_cases['Date'], df_merged_cases['New_Cases'], label='Actual New Cases')\n","plt.plot(df_merged_cases['Date'], df_merged_cases['yhat'], label='Predicted New Cases')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (Log Transformed)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['New_Deaths'], label='Actual New Deaths')\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['yhat'], label='Predicted New Deaths')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (Log Transformed)')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:09:19.395957Z","iopub.status.busy":"2024-10-13T07:09:19.395341Z","iopub.status.idle":"2024-10-13T07:09:30.138009Z","shell.execute_reply":"2024-10-13T07:09:30.136935Z","shell.execute_reply.started":"2024-10-13T07:09:19.395911Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from prophet import Prophet\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","# Convert the 'Date' column to datetime\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","\n","# Scaling the New Cases and New Deaths using StandardScaler\n","scaler_cases = StandardScaler()\n","scaler_deaths = StandardScaler()\n","\n","# Add scaled values\n","data['Scaled_New_Cases'] = scaler_cases.fit_transform(data[['New_Cases']])\n","data['Scaled_New_Deaths'] = scaler_deaths.fit_transform(data[['New_Deaths']])\n","\n","# Add a floor to prevent negative predictions\n","data['floor'] = 0\n","\n","# Define custom seasonality (e.g., bi-weekly)\n","model_cases = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.1,  # Increased to better capture trend changes\n","    seasonality_prior_scale=1.0,  # Lowered to reduce overfitting\n","    holidays=pd.DataFrame({\n","        'holiday': 'health_intervention',\n","        'ds': pd.to_datetime(['2022-06-15', '2022-09-01']),  # Example dates\n","        'lower_window': 0,\n","        'upper_window': 5,\n","    })\n",").add_seasonality(name='biweekly', period=14, fourier_order=8)  # Increased Fourier order for better seasonality capture\n","\n","# Similarly define model for New Deaths\n","model_deaths = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.1,  # Increased to better capture trend changes\n","    seasonality_prior_scale=1.0,  # Lowered to reduce overfitting\n","    holidays=pd.DataFrame({\n","        'holiday': 'health_intervention',\n","        'ds': pd.to_datetime(['2022-06-15', '2022-09-01']),  # Example dates\n","        'lower_window': 0,\n","        'upper_window': 5,\n","    })\n",")\n","\n","# Fit the models\n","model_cases.fit(data[['Date', 'Scaled_New_Cases', 'floor']].rename(columns={'Date': 'ds', 'Scaled_New_Cases': 'y'}))\n","model_deaths.fit(data[['Date', 'Scaled_New_Deaths', 'floor']].rename(columns={'Date': 'ds', 'Scaled_New_Deaths': 'y'}))\n","\n","# Generate future dates for prediction\n","future_cases = model_cases.make_future_dataframe(periods=60)\n","future_cases['floor'] = 0  # Add the floor constraint to future data\n","future_deaths = model_deaths.make_future_dataframe(periods=60)\n","future_deaths['floor'] = 0\n","\n","# Forecasting\n","forecast_cases = model_cases.predict(future_cases)\n","forecast_deaths = model_deaths.predict(future_deaths)\n","\n","# Inverse scaling to bring predictions back to original scale\n","forecast_cases['yhat'] = scaler_cases.inverse_transform(forecast_cases[['yhat']])\n","forecast_deaths['yhat'] = scaler_deaths.inverse_transform(forecast_deaths[['yhat']])\n","\n","# Handle negative predictions by replacing them with zero\n","forecast_cases['yhat'] = np.where(forecast_cases['yhat'] < 0, 0, forecast_cases['yhat'])\n","forecast_deaths['yhat'] = np.where(forecast_deaths['yhat'] < 0, 0, forecast_deaths['yhat'])\n","\n","# Merge actual and predicted values for evaluation (New Cases)\n","df_merged_cases = pd.merge(data[['Date', 'New_Cases']], forecast_cases[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_cases.dropna(inplace=True)\n","\n","# Merge actual and predicted values for evaluation (New Deaths)\n","df_merged_deaths = pd.merge(data[['Date', 'New_Deaths']], forecast_deaths[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_deaths.dropna(inplace=True)\n","\n","# Evaluation metrics for New Cases\n","rmse_cases = np.sqrt(mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat']))\n","mse_cases = mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","mae_cases = mean_absolute_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","r2_cases = r2_score(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","\n","# Evaluation metrics for New Deaths\n","rmse_deaths = np.sqrt(mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat']))\n","mse_deaths = mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","mae_deaths = mean_absolute_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","r2_deaths = r2_score(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","\n","# Print evaluation metrics\n","print(f\"New Cases - RMSE: {rmse_cases}, MSE: {mse_cases}, MAE: {mae_cases}, R-squared: {r2_cases}\")\n","print(f\"New Deaths - RMSE: {rmse_deaths}, MSE: {mse_deaths}, MAE: {mae_deaths}, R-squared: {r2_deaths}\")\n","\n","# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_cases['Date'], df_merged_cases['New_Cases'], label='Actual New Cases')\n","plt.plot(df_merged_cases['Date'], df_merged_cases['yhat'], label='Predicted New Cases')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (with Custom Seasonalities & Holidays)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['New_Deaths'], label='Actual New Deaths')\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['yhat'], label='Predicted New Deaths')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (with Custom Seasonalities & Holidays)')\n","plt.legend()\n","plt.show()\n","\n","# Optional: Perform cross-validation to ensure robustness of the model\n","from prophet.diagnostics import cross_validation, performance_metrics\n","\n","df_cv_cases = cross_validation(model_cases, initial='365 days', period='180 days', horizon='60 days')\n","df_p_cases = performance_metrics(df_cv_cases)\n","print(df_p_cases.head())\n","\n","df_cv_deaths = cross_validation(model_deaths, initial='365 days', period='180 days', horizon='60 days')\n","df_p_deaths = performance_metrics(df_cv_deaths)\n","print(df_p_deaths.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:10:05.776225Z","iopub.status.busy":"2024-10-13T07:10:05.775743Z","iopub.status.idle":"2024-10-13T07:10:18.799670Z","shell.execute_reply":"2024-10-13T07:10:18.798257Z","shell.execute_reply.started":"2024-10-13T07:10:05.776180Z"},"trusted":true},"outputs":[],"source":["# Install required libraries\n","!pip install prophet scikit-learn matplotlib\n","\n","import pandas as pd\n","from prophet import Prophet\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","# Convert the 'Date' column to datetime\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","\n","# Remove outliers: apply thresholding to remove extreme outliers (for both New_Cases and New_Deaths)\n","q_low_cases = data['New_Cases'].quantile(0.01)\n","q_high_cases = data['New_Cases'].quantile(0.99)\n","q_low_deaths = data['New_Deaths'].quantile(0.01)\n","q_high_deaths = data['New_Deaths'].quantile(0.99)\n","\n","data = data[(data['New_Cases'] > q_low_cases) & (data['New_Cases'] < q_high_cases)]\n","data = data[(data['New_Deaths'] > q_low_deaths) & (data['New_Deaths'] < q_high_deaths)]\n","\n","# Scaling the New Cases and New Deaths using StandardScaler\n","scaler_cases = StandardScaler()\n","scaler_deaths = StandardScaler()\n","\n","# Add scaled values\n","data['Scaled_New_Cases'] = scaler_cases.fit_transform(data[['New_Cases']])\n","data['Scaled_New_Deaths'] = scaler_deaths.fit_transform(data[['New_Deaths']])\n","\n","# Initialize Prophet model with higher flexibility (lower changepoint_prior_scale) and adjusted fourier_order for seasonality\n","model_cases = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.01,  # Lower value for more flexibility\n","    seasonality_prior_scale=10.0\n",")\n","\n","# Fine-tuning seasonality Fourier order\n","model_cases.add_seasonality(name='biweekly', period=14, fourier_order=7)\n","\n","# Adding custom holidays for any impactful event dates (e.g., health interventions)\n","holidays = pd.DataFrame({\n","  'holiday': 'health_intervention',\n","  'ds': pd.to_datetime(['2022-06-15', '2022-09-01']),  # Example dates\n","  'lower_window': 0,\n","  'upper_window': 5,\n","})\n","\n","model_cases = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.01,\n","    seasonality_prior_scale=10.0,\n","    holidays=holidays\n",")\n","\n","model_deaths = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.01,\n","    seasonality_prior_scale=10.0,\n","    holidays=holidays\n",")\n","\n","# Fit the models\n","model_cases.fit(data[['Date', 'Scaled_New_Cases']].rename(columns={'Date': 'ds', 'Scaled_New_Cases': 'y'}))\n","model_deaths.fit(data[['Date', 'Scaled_New_Deaths']].rename(columns={'Date': 'ds', 'Scaled_New_Deaths': 'y'}))\n","\n","# Generate future dates for prediction\n","future_cases = model_cases.make_future_dataframe(periods=60)\n","future_deaths = model_deaths.make_future_dataframe(periods=60)\n","\n","# Forecasting\n","forecast_cases = model_cases.predict(future_cases)\n","forecast_deaths = model_deaths.predict(future_deaths)\n","\n","# Inverse scaling to bring predictions back to original scale\n","forecast_cases['yhat'] = scaler_cases.inverse_transform(forecast_cases[['yhat']])\n","forecast_deaths['yhat'] = scaler_deaths.inverse_transform(forecast_deaths[['yhat']])\n","\n","# Merge actual and predicted values for evaluation (New Cases)\n","df_merged_cases = pd.merge(data[['Date', 'New_Cases']], forecast_cases[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_cases.dropna(inplace=True)\n","\n","# Merge actual and predicted values for evaluation (New Deaths)\n","df_merged_deaths = pd.merge(data[['Date', 'New_Deaths']], forecast_deaths[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_deaths.dropna(inplace=True)\n","\n","# Evaluation metrics for New Cases\n","rmse_cases = np.sqrt(mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat']))\n","mse_cases = mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","mae_cases = mean_absolute_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","r2_cases = r2_score(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","\n","# Evaluation metrics for New Deaths\n","rmse_deaths = np.sqrt(mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat']))\n","mse_deaths = mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","mae_deaths = mean_absolute_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","r2_deaths = r2_score(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","\n","# Print evaluation metrics\n","print(f\"New Cases - RMSE: {rmse_cases}, MSE: {mse_cases}, MAE: {mae_cases}, R-squared: {r2_cases}\")\n","print(f\"New Deaths - RMSE: {rmse_deaths}, MSE: {mse_deaths}, MAE: {mae_deaths}, R-squared: {r2_deaths}\")\n","\n","# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_cases['Date'], df_merged_cases['New_Cases'], label='Actual New Cases')\n","plt.plot(df_merged_cases['Date'], df_merged_cases['yhat'], label='Predicted New Cases')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (After Fine-Tuning)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['New_Deaths'], label='Actual New Deaths')\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['yhat'], label='Predicted New Deaths')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (After Fine-Tuning)')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:12:12.581202Z","iopub.status.busy":"2024-10-13T07:12:12.580589Z","iopub.status.idle":"2024-10-13T07:12:25.311697Z","shell.execute_reply":"2024-10-13T07:12:25.310639Z","shell.execute_reply.started":"2024-10-13T07:12:12.581140Z"},"trusted":true},"outputs":[],"source":["# Install required libraries\n","!pip install prophet scikit-learn matplotlib\n","\n","import pandas as pd\n","from prophet import Prophet\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","# Convert the 'Date' column to datetime\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","\n","# Remove outliers: Apply thresholding to remove extreme outliers\n","q_low_cases = data['New_Cases'].quantile(0.01)\n","q_high_cases = data['New_Cases'].quantile(0.99)\n","q_low_deaths = data['New_Deaths'].quantile(0.01)\n","q_high_deaths = data['New_Deaths'].quantile(0.99)\n","\n","data = data[(data['New_Cases'] > q_low_cases) & (data['New_Cases'] < q_high_cases)]\n","data = data[(data['New_Deaths'] > q_low_deaths) & (data['New_Deaths'] < q_high_deaths)]\n","\n","# Add autoregressive features: Previous days values as a regressor\n","data['lag_1_cases'] = data['New_Cases'].shift(1).fillna(0)\n","data['lag_1_deaths'] = data['New_Deaths'].shift(1).fillna(0)\n","\n","# Scaling the New Cases and New Deaths using StandardScaler\n","scaler_cases = StandardScaler()\n","scaler_deaths = StandardScaler()\n","\n","data['Scaled_New_Cases'] = scaler_cases.fit_transform(data[['New_Cases']])\n","data['Scaled_New_Deaths'] = scaler_deaths.fit_transform(data[['New_Deaths']])\n","\n","# Initialize Prophet models with autoregressive feature as a regressor\n","model_cases = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.01,\n","    seasonality_prior_scale=10.0\n",")\n","model_cases.add_regressor('lag_1_cases')\n","\n","model_deaths = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.01,\n","    seasonality_prior_scale=10.0\n",")\n","model_deaths.add_regressor('lag_1_deaths')\n","\n","# Fit the models\n","model_cases.fit(data[['Date', 'Scaled_New_Cases', 'lag_1_cases']].rename(columns={'Date': 'ds', 'Scaled_New_Cases': 'y'}))\n","model_deaths.fit(data[['Date', 'Scaled_New_Deaths', 'lag_1_deaths']].rename(columns={'Date': 'ds', 'Scaled_New_Deaths': 'y'}))\n","\n","# Generate future dates and use last lag value for predictions\n","future_cases = model_cases.make_future_dataframe(periods=60)\n","future_cases['lag_1_cases'] = data['lag_1_cases'].iloc[-1]\n","\n","future_deaths = model_deaths.make_future_dataframe(periods=60)\n","future_deaths['lag_1_deaths'] = data['lag_1_deaths'].iloc[-1]\n","\n","# Forecasting\n","forecast_cases = model_cases.predict(future_cases)\n","forecast_deaths = model_deaths.predict(future_deaths)\n","\n","# Inverse scaling and clip to avoid negative predictions\n","forecast_cases['yhat'] = scaler_cases.inverse_transform(forecast_cases[['yhat']])\n","forecast_deaths['yhat'] = scaler_deaths.inverse_transform(forecast_deaths[['yhat']])\n","\n","forecast_cases['yhat'] = forecast_cases['yhat'].clip(lower=0)  # Clip negative values to 0\n","forecast_deaths['yhat'] = forecast_deaths['yhat'].clip(lower=0)\n","\n","# Merge actual and predicted values for evaluation\n","df_merged_cases = pd.merge(data[['Date', 'New_Cases']], forecast_cases[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_cases.dropna(inplace=True)\n","\n","df_merged_deaths = pd.merge(data[['Date', 'New_Deaths']], forecast_deaths[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_deaths.dropna(inplace=True)\n","\n","# Evaluation metrics for New Cases\n","rmse_cases = np.sqrt(mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat']))\n","mae_cases = mean_absolute_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","r2_cases = r2_score(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","\n","# Evaluation metrics for New Deaths\n","rmse_deaths = np.sqrt(mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat']))\n","mae_deaths = mean_absolute_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","r2_deaths = r2_score(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","\n","# Print evaluation metrics\n","print(f\"New Cases - RMSE: {rmse_cases}, MAE: {mae_cases}, R-squared: {r2_cases}\")\n","print(f\"New Deaths - RMSE: {rmse_deaths}, MAE: {mae_deaths}, R-squared: {r2_deaths}\")\n","\n","# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_cases['Date'], df_merged_cases['New_Cases'], label='Actual New Cases')\n","plt.plot(df_merged_cases['Date'], df_merged_cases['yhat'], label='Predicted New Cases')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (With AR Features)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['New_Deaths'], label='Actual New Deaths')\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['yhat'], label='Predicted New Deaths')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (With AR Features)')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:14:47.014743Z","iopub.status.busy":"2024-10-13T07:14:47.014011Z","iopub.status.idle":"2024-10-13T07:14:59.782397Z","shell.execute_reply":"2024-10-13T07:14:59.781221Z","shell.execute_reply.started":"2024-10-13T07:14:47.014701Z"},"trusted":true},"outputs":[],"source":["# Install required libraries\n","!pip install prophet scikit-learn matplotlib\n","\n","import pandas as pd\n","from prophet import Prophet\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","# Convert the 'Date' column to datetime\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","\n","# Remove outliers: apply thresholding to remove extreme outliers (for both New_Cases and New_Deaths)\n","q_low_cases = data['New_Cases'].quantile(0.01)\n","q_high_cases = data['New_Cases'].quantile(0.99)\n","q_low_deaths = data['New_Deaths'].quantile(0.01)\n","q_high_deaths = data['New_Deaths'].quantile(0.99)\n","\n","data = data[(data['New_Cases'] > q_low_cases) & (data['New_Cases'] < q_high_cases)]\n","data = data[(data['New_Deaths'] > q_low_deaths) & (data['New_Deaths'] < q_high_deaths)]\n","\n","# Add autoregressive features: previous days values as regressors (lag_1, lag_2)\n","data['lag_1_cases'] = data['New_Cases'].shift(1).fillna(0)\n","data['lag_2_cases'] = data['New_Cases'].shift(2).fillna(0)\n","\n","data['lag_1_deaths'] = data['New_Deaths'].shift(1).fillna(0)\n","data['lag_2_deaths'] = data['New_Deaths'].shift(2).fillna(0)\n","\n","# Scaling the New Cases and New Deaths using StandardScaler\n","scaler_cases = StandardScaler()\n","scaler_deaths = StandardScaler()\n","\n","# Add scaled values\n","data['Scaled_New_Cases'] = scaler_cases.fit_transform(data[['New_Cases']])\n","data['Scaled_New_Deaths'] = scaler_deaths.fit_transform(data[['New_Deaths']])\n","\n","# Initialize Prophet models with autoregressive features as regressors\n","model_cases = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.01,\n","    seasonality_prior_scale=10.0\n",")\n","model_cases.add_regressor('lag_1_cases')  # Adding autoregressive feature lag_1\n","model_cases.add_regressor('lag_2_cases')  # Adding autoregressive feature lag_2\n","\n","model_deaths = Prophet(\n","    seasonality_mode='multiplicative',\n","    yearly_seasonality=True,\n","    weekly_seasonality=True,\n","    changepoint_prior_scale=0.01,\n","    seasonality_prior_scale=10.0\n",")\n","model_deaths.add_regressor('lag_1_deaths')  # Adding autoregressive feature lag_1\n","model_deaths.add_regressor('lag_2_deaths')  # Adding autoregressive feature lag_2\n","\n","# Fit the models\n","model_cases.fit(data[['Date', 'Scaled_New_Cases', 'lag_1_cases', 'lag_2_cases']].rename(columns={'Date': 'ds', 'Scaled_New_Cases': 'y'}))\n","model_deaths.fit(data[['Date', 'Scaled_New_Deaths', 'lag_1_deaths', 'lag_2_deaths']].rename(columns={'Date': 'ds', 'Scaled_New_Deaths': 'y'}))\n","\n","# Generate future dates for prediction and add autoregressive features (lag_1_cases, lag_2_cases)\n","future_cases = model_cases.make_future_dataframe(periods=60)\n","future_cases['lag_1_cases'] = data['lag_1_cases'].iloc[-1]  # Using last value for lag_1_cases\n","future_cases['lag_2_cases'] = data['lag_2_cases'].iloc[-1]  # Using last value for lag_2_cases\n","\n","future_deaths = model_deaths.make_future_dataframe(periods=60)\n","future_deaths['lag_1_deaths'] = data['lag_1_deaths'].iloc[-1]  # Using last value for lag_1_deaths\n","future_deaths['lag_2_deaths'] = data['lag_2_deaths'].iloc[-1]  # Using last value for lag_2_deaths\n","\n","# Forecasting\n","forecast_cases = model_cases.predict(future_cases)\n","forecast_deaths = model_deaths.predict(future_deaths)\n","\n","# Inverse scaling to bring predictions back to original scale\n","forecast_cases['yhat'] = scaler_cases.inverse_transform(forecast_cases[['yhat']])\n","forecast_deaths['yhat'] = scaler_deaths.inverse_transform(forecast_deaths[['yhat']])\n","\n","# Merge actual and predicted values for evaluation (New Cases)\n","df_merged_cases = pd.merge(data[['Date', 'New_Cases']], forecast_cases[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_cases.dropna(inplace=True)\n","\n","# Merge actual and predicted values for evaluation (New Deaths)\n","df_merged_deaths = pd.merge(data[['Date', 'New_Deaths']], forecast_deaths[['ds', 'yhat']], left_on='Date', right_on='ds', how='left')\n","df_merged_deaths.dropna(inplace=True)\n","\n","# Evaluation metrics for New Cases\n","rmse_cases = np.sqrt(mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat']))\n","mse_cases = mean_squared_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","mae_cases = mean_absolute_error(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","r2_cases = r2_score(df_merged_cases['New_Cases'], df_merged_cases['yhat'])\n","\n","# Evaluation metrics for New Deaths\n","rmse_deaths = np.sqrt(mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat']))\n","mse_deaths = mean_squared_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","mae_deaths = mean_absolute_error(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","r2_deaths = r2_score(df_merged_deaths['New_Deaths'], df_merged_deaths['yhat'])\n","\n","# Print evaluation metrics\n","print(f\"New Cases - RMSE: {rmse_cases}, MSE: {mse_cases}, MAE: {mae_cases}, R-squared: {r2_cases}\")\n","print(f\"New Deaths - RMSE: {rmse_deaths}, MSE: {mse_deaths}, MAE: {mae_deaths}, R-squared: {r2_deaths}\")\n","\n","# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_cases['Date'], df_merged_cases['New_Cases'], label='Actual New Cases')\n","plt.plot(df_merged_cases['Date'], df_merged_cases['yhat'], label='Predicted New Cases')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (With Lag 1 and Lag 2 AR Features)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['New_Deaths'], label='Actual New Deaths')\n","plt.plot(df_merged_deaths['Date'], df_merged_deaths['yhat'], label='Predicted New Deaths')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (With Lag 1 and Lag 2 AR Features)')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**ENsumblw with prophet**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:33:13.169119Z","iopub.status.busy":"2024-10-13T07:33:13.167086Z","iopub.status.idle":"2024-10-13T07:33:24.249867Z","shell.execute_reply":"2024-10-13T07:33:24.248372Z","shell.execute_reply.started":"2024-10-13T07:33:13.169005Z"},"trusted":true},"outputs":[],"source":["!pip install prophet pmdarima xgboost scikit-learn matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:33:36.778639Z","iopub.status.busy":"2024-10-13T07:33:36.777909Z","iopub.status.idle":"2024-10-13T07:33:36.798996Z","shell.execute_reply":"2024-10-13T07:33:36.798112Z","shell.execute_reply.started":"2024-10-13T07:33:36.778563Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from prophet import Prophet\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from xgboost import XGBRegressor\n","from sklearn.model_selection import train_test_split\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","\n","# Prepare data for modeling\n","data['lag_1_cases'] = data['New_Cases'].shift(1).fillna(0)\n","data['lag_2_cases'] = data['New_Cases'].shift(2).fillna(0)\n","data['lag_1_deaths'] = data['New_Deaths'].shift(1).fillna(0)\n","data['lag_2_deaths'] = data['New_Deaths'].shift(2).fillna(0)\n","\n","# Drop rows with missing values\n","data = data.dropna()\n","\n","# Split data into training and testing sets\n","train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T07:33:41.889493Z","iopub.status.busy":"2024-10-13T07:33:41.888532Z","iopub.status.idle":"2024-10-13T07:33:43.237200Z","shell.execute_reply":"2024-10-13T07:33:43.236230Z","shell.execute_reply.started":"2024-10-13T07:33:41.889445Z"},"trusted":true},"outputs":[],"source":["# Prophet Model for New Cases\n","prophet_cases = Prophet(seasonality_mode='multiplicative', yearly_seasonality=True, weekly_seasonality=True)\n","prophet_cases.fit(train_data[['Date', 'New_Cases']].rename(columns={'Date': 'ds', 'New_Cases': 'y'}))\n","\n","# Prophet Model for New Deaths\n","prophet_deaths = Prophet(seasonality_mode='multiplicative', yearly_seasonality=True, weekly_seasonality=True)\n","prophet_deaths.fit(train_data[['Date', 'New_Deaths']].rename(columns={'Date': 'ds', 'New_Deaths': 'y'}))\n","\n","# Make future predictions for Prophet\n","future_dates = prophet_cases.make_future_dataframe(periods=len(test_data), freq='D')\n","forecast_cases = prophet_cases.predict(future_dates)\n","forecast_deaths = prophet_deaths.predict(future_dates)\n","\n","# Get the predicted values from Prophet\n","prophet_cases_preds = forecast_cases[['ds', 'yhat']].tail(len(test_data))['yhat'].values\n","prophet_deaths_preds = forecast_deaths[['ds', 'yhat']].tail(len(test_data))['yhat'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:02:38.461324Z","iopub.status.busy":"2024-10-13T09:02:38.460962Z","iopub.status.idle":"2024-10-13T09:20:08.434217Z","shell.execute_reply":"2024-10-13T09:20:08.433043Z","shell.execute_reply.started":"2024-10-13T09:02:38.461287Z"},"trusted":true},"outputs":[],"source":["# SARIMA for New Cases\n","sarima_cases = auto_arima(train_data['New_Cases'], seasonal=True, stepwise=True)\n","sarima_cases_preds = sarima_cases.predict(n_periods=len(test_data))\n","\n","# SARIMA for New Deaths\n","sarima_deaths = auto_arima(train_data['New_Deaths'], seasonal=True, stepwise=True)\n","sarima_deaths_preds = sarima_deaths.predict(n_periods=len(test_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:20:09.593449Z","iopub.status.busy":"2024-10-13T09:20:09.593046Z","iopub.status.idle":"2024-10-13T09:20:09.694497Z","shell.execute_reply":"2024-10-13T09:20:09.693488Z","shell.execute_reply.started":"2024-10-13T09:20:09.593408Z"},"trusted":true},"outputs":[],"source":["# XGBoost for New Cases\n","xgb_cases = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)\n","xgb_cases.fit(train_data[['lag_1_cases', 'lag_2_cases']], train_data['New_Cases'])\n","xgb_cases_preds = xgb_cases.predict(test_data[['lag_1_cases', 'lag_2_cases']])\n","\n","# XGBoost for New Deaths\n","xgb_deaths = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)\n","xgb_deaths.fit(train_data[['lag_1_deaths', 'lag_2_deaths']], train_data['New_Deaths'])\n","xgb_deaths_preds = xgb_deaths.predict(test_data[['lag_1_deaths', 'lag_2_deaths']])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:22:03.109168Z","iopub.status.busy":"2024-10-13T09:22:03.108687Z","iopub.status.idle":"2024-10-13T09:22:03.124334Z","shell.execute_reply":"2024-10-13T09:22:03.123127Z","shell.execute_reply.started":"2024-10-13T09:22:03.109125Z"},"trusted":true},"outputs":[],"source":["# Combining predictions for New Cases\n","final_cases_preds = 0.4 * prophet_cases_preds + 0.3 * sarima_cases_preds + 0.3 * xgb_cases_preds\n","\n","# Combining predictions for New Deaths\n","final_deaths_preds = 0.4 * prophet_deaths_preds + 0.3 * sarima_deaths_preds + 0.3 * xgb_deaths_preds\n","\n","# Evaluation metrics for New Cases\n","rmse_cases = np.sqrt(mean_squared_error(test_data['New_Cases'], final_cases_preds))\n","mse_cases = mean_squared_error(test_data['New_Cases'], final_cases_preds)\n","mae_cases = mean_absolute_error(test_data['New_Cases'], final_cases_preds)\n","r2_cases = r2_score(test_data['New_Cases'], final_cases_preds)\n","\n","# Evaluation metrics for New Deaths\n","rmse_deaths = np.sqrt(mean_squared_error(test_data['New_Deaths'], final_deaths_preds))\n","mse_deaths = mean_squared_error(test_data['New_Deaths'], final_deaths_preds)\n","mae_deaths = mean_absolute_error(test_data['New_Deaths'], final_deaths_preds)\n","r2_deaths = r2_score(test_data['New_Deaths'], final_deaths_preds)\n","\n","# Print results\n","print(f\"New Cases - RMSE: {rmse_cases}, MSE: {mse_cases}, MAE: {mae_cases}, R-squared: {r2_cases}\")\n","print(f\"New Deaths - RMSE: {rmse_deaths}, MSE: {mse_deaths}, MAE: {mae_deaths}, R-squared: {r2_deaths}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:22:54.825659Z","iopub.status.busy":"2024-10-13T09:22:54.824758Z","iopub.status.idle":"2024-10-13T09:22:55.616394Z","shell.execute_reply":"2024-10-13T09:22:55.615319Z","shell.execute_reply.started":"2024-10-13T09:22:54.825613Z"},"trusted":true},"outputs":[],"source":["# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(test_data['Date'], test_data['New_Cases'], label='Actual New Cases')\n","plt.plot(test_data['Date'], final_cases_preds, label='Predicted New Cases (Ensemble)')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (Ensemble)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(test_data['Date'], test_data['New_Deaths'], label='Actual New Deaths')\n","plt.plot(test_data['Date'], final_deaths_preds, label='Predicted New Deaths (Ensemble)')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (Ensemble)')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:23:14.172291Z","iopub.status.busy":"2024-10-13T09:23:14.171313Z","iopub.status.idle":"2024-10-13T09:40:43.416742Z","shell.execute_reply":"2024-10-13T09:40:43.415485Z","shell.execute_reply.started":"2024-10-13T09:23:14.172246Z"},"trusted":true},"outputs":[],"source":["from pmdarima import auto_arima\n","\n","# Auto ARIMA for New Cases\n","sarima_cases = auto_arima(train_data['New_Cases'], seasonal=True, stepwise=True, trace=True)\n","\n","# Auto ARIMA for New Deaths\n","sarima_deaths = auto_arima(train_data['New_Deaths'], seasonal=True, stepwise=True, trace=True)\n","\n","# SARIMA Predictions\n","sarima_cases_preds = sarima_cases.predict(n_periods=len(test_data))\n","sarima_deaths_preds = sarima_deaths.predict(n_periods=len(test_data))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:50:56.197314Z","iopub.status.busy":"2024-10-13T09:50:56.195407Z","iopub.status.idle":"2024-10-13T09:51:09.209414Z","shell.execute_reply":"2024-10-13T09:51:09.207934Z","shell.execute_reply.started":"2024-10-13T09:50:56.197255Z"},"trusted":true},"outputs":[],"source":["from xgboost import XGBRegressor\n","from sklearn.model_selection import GridSearchCV\n","\n","# Prepare the training data for XGBoost\n","X_train_cases = train_data[['lag_1_cases', 'lag_2_cases']]\n","y_train_cases = train_data['New_Cases']\n","X_train_deaths = train_data[['lag_1_deaths', 'lag_2_deaths']]\n","y_train_deaths = train_data['New_Deaths']\n","\n","# Define parameter grid for tuning\n","param_grid = {\n","    'n_estimators': [50, 100, 150],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'max_depth': [3, 5, 7]\n","}\n","\n","# Hyperparameter tuning for XGBoost on New Cases\n","xgb_cases = XGBRegressor()\n","grid_cases = GridSearchCV(xgb_cases, param_grid, cv=5, scoring='neg_mean_squared_error')\n","grid_cases.fit(X_train_cases, y_train_cases)\n","best_xgb_cases = grid_cases.best_estimator_\n","\n","# Hyperparameter tuning for XGBoost on New Deaths\n","xgb_deaths = XGBRegressor()\n","grid_deaths = GridSearchCV(xgb_deaths, param_grid, cv=5, scoring='neg_mean_squared_error')\n","grid_deaths.fit(X_train_deaths, y_train_deaths)\n","best_xgb_deaths = grid_deaths.best_estimator_\n","\n","# Make predictions\n","xgb_cases_preds = best_xgb_cases.predict(test_data[['lag_1_cases', 'lag_2_cases']])\n","xgb_deaths_preds = best_xgb_deaths.predict(test_data[['lag_1_deaths', 'lag_2_deaths']])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:51:39.017596Z","iopub.status.busy":"2024-10-13T09:51:39.016778Z","iopub.status.idle":"2024-10-13T09:51:40.460426Z","shell.execute_reply":"2024-10-13T09:51:40.459152Z","shell.execute_reply.started":"2024-10-13T09:51:39.017546Z"},"trusted":true},"outputs":[],"source":["# Prophet Model for New Cases\n","prophet_cases = Prophet(seasonality_mode='multiplicative', yearly_seasonality=True, weekly_seasonality=True)\n","prophet_cases.fit(train_data[['Date', 'New_Cases']].rename(columns={'Date': 'ds', 'New_Cases': 'y'}))\n","\n","# Prophet Model for New Deaths\n","prophet_deaths = Prophet(seasonality_mode='multiplicative', yearly_seasonality=True, weekly_seasonality=True)\n","prophet_deaths.fit(train_data[['Date', 'New_Deaths']].rename(columns={'Date': 'ds', 'New_Deaths': 'y'}))\n","\n","# Make future predictions for Prophet\n","future_dates = prophet_cases.make_future_dataframe(periods=len(test_data), freq='D')\n","forecast_cases = prophet_cases.predict(future_dates)\n","forecast_deaths = prophet_deaths.predict(future_dates)\n","\n","# Get the predicted values from Prophet\n","prophet_cases_preds = forecast_cases[['ds', 'yhat']].tail(len(test_data))['yhat'].values\n","prophet_deaths_preds = forecast_deaths[['ds', 'yhat']].tail(len(test_data))['yhat'].values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:51:43.304960Z","iopub.status.busy":"2024-10-13T09:51:43.304536Z","iopub.status.idle":"2024-10-13T09:51:44.649598Z","shell.execute_reply":"2024-10-13T09:51:44.648312Z","shell.execute_reply.started":"2024-10-13T09:51:43.304922Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import StackingRegressor\n","from sklearn.linear_model import Ridge\n","\n","# First, make predictions with SARIMA and treat them as features\n","sarima_cases_preds = sarima_cases.predict(n_periods=len(test_data))\n","sarima_deaths_preds = sarima_deaths.predict(n_periods=len(test_data))\n","\n","# Prepare the stacking features (SARIMA predictions are treated as features)\n","X_train_cases_stacking = np.column_stack([prophet_cases_preds, xgb_cases_preds, sarima_cases_preds])\n","X_train_deaths_stacking = np.column_stack([prophet_deaths_preds, xgb_deaths_preds, sarima_deaths_preds])\n","\n","# Prepare the stacking regressor (Prophet, XGBoost are used as regressors)\n","stacking_cases = StackingRegressor(\n","    estimators=[\n","        ('prophet', best_xgb_cases),  # XGBoost model\n","        ('xgb', best_xgb_cases),  # XGBoost model\n","    ],\n","    final_estimator=Ridge()\n",")\n","\n","stacking_deaths = StackingRegressor(\n","    estimators=[\n","        ('prophet', best_xgb_deaths),  # XGBoost model\n","        ('xgb', best_xgb_deaths),  # XGBoost model\n","    ],\n","    final_estimator=Ridge()\n",")\n","\n","# Train the stacking model using SARIMA predictions as features\n","stacking_cases.fit(X_train_cases_stacking, test_data['New_Cases'])\n","stacking_deaths.fit(X_train_deaths_stacking, test_data['New_Deaths'])\n","\n","# Make predictions with the stacking model\n","stacked_cases_preds = stacking_cases.predict(X_train_cases_stacking)\n","stacked_deaths_preds = stacking_deaths.predict(X_train_deaths_stacking)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:51:51.152961Z","iopub.status.busy":"2024-10-13T09:51:51.152033Z","iopub.status.idle":"2024-10-13T09:51:51.166379Z","shell.execute_reply":"2024-10-13T09:51:51.165030Z","shell.execute_reply.started":"2024-10-13T09:51:51.152909Z"},"trusted":true},"outputs":[],"source":["# Evaluation metrics for New Cases\n","rmse_cases = np.sqrt(mean_squared_error(test_data['New_Cases'], stacked_cases_preds))\n","mse_cases = mean_squared_error(test_data['New_Cases'], stacked_cases_preds)\n","mae_cases = mean_absolute_error(test_data['New_Cases'], stacked_cases_preds)\n","r2_cases = r2_score(test_data['New_Cases'], stacked_cases_preds)\n","\n","# Evaluation metrics for New Deaths\n","rmse_deaths = np.sqrt(mean_squared_error(test_data['New_Deaths'], stacked_deaths_preds))\n","mse_deaths = mean_squared_error(test_data['New_Deaths'], stacked_deaths_preds)\n","mae_deaths = mean_absolute_error(test_data['New_Deaths'], stacked_deaths_preds)\n","r2_deaths = r2_score(test_data['New_Deaths'], stacked_deaths_preds)\n","\n","# Print results\n","print(f\"New Cases - RMSE: {rmse_cases}, MSE: {mse_cases}, MAE: {mae_cases}, R-squared: {r2_cases}\")\n","print(f\"New Deaths - RMSE: {rmse_deaths}, MSE: {mse_deaths}, MAE: {mae_deaths}, R-squared: {r2_deaths}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:51:55.792167Z","iopub.status.busy":"2024-10-13T09:51:55.791009Z","iopub.status.idle":"2024-10-13T09:51:56.655662Z","shell.execute_reply":"2024-10-13T09:51:56.654415Z","shell.execute_reply.started":"2024-10-13T09:51:55.792113Z"},"trusted":true},"outputs":[],"source":["# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(test_data['Date'], test_data['New_Cases'], label='Actual New Cases')\n","plt.plot(test_data['Date'], stacked_cases_preds, label='Predicted New Cases (Ensemble)')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (Stacking Ensemble)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(test_data['Date'], test_data['New_Deaths'], label='Actual New Deaths')\n","plt.plot(test_data['Date'], stacked_deaths_preds, label='Predicted New Deaths (Stacking Ensemble)')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (Stacking Ensemble)')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**new ensamble**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:52:07.891221Z","iopub.status.busy":"2024-10-13T09:52:07.890745Z","iopub.status.idle":"2024-10-13T09:52:07.946750Z","shell.execute_reply":"2024-10-13T09:52:07.945484Z","shell.execute_reply.started":"2024-10-13T09:52:07.891172Z"},"trusted":true},"outputs":[],"source":["# Necessary imports\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","\n","# Remove outliers based on quantiles\n","q_low_cases = data['New_Cases'].quantile(0.01)\n","q_high_cases = data['New_Cases'].quantile(0.99)\n","q_low_deaths = data['New_Deaths'].quantile(0.01)\n","q_high_deaths = data['New_Deaths'].quantile(0.99)\n","\n","data = data[(data['New_Cases'] > q_low_cases) & (data['New_Cases'] < q_high_cases)]\n","data = data[(data['New_Deaths'] > q_low_deaths) & (data['New_Deaths'] < q_high_deaths)]\n","\n","# Add lag features for short-term dependencies\n","data['lag_1_cases'] = data['New_Cases'].shift(1).fillna(0)\n","data['lag_2_cases'] = data['New_Cases'].shift(2).fillna(0)\n","data['lag_1_deaths'] = data['New_Deaths'].shift(1).fillna(0)\n","data['lag_2_deaths'] = data['New_Deaths'].shift(2).fillna(0)\n","\n","# Split the data into training and testing sets\n","train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)\n","\n","# Display first few rows of the training data\n","train_data.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:52:11.160150Z","iopub.status.busy":"2024-10-13T09:52:11.159223Z","iopub.status.idle":"2024-10-13T09:52:11.909899Z","shell.execute_reply":"2024-10-13T09:52:11.908482Z","shell.execute_reply.started":"2024-10-13T09:52:11.160098Z"},"trusted":true},"outputs":[],"source":["from prophet import Prophet\n","\n","# Prophet Model for New Cases\n","prophet_cases = Prophet(yearly_seasonality=True, weekly_seasonality=True, changepoint_prior_scale=0.01)\n","prophet_cases.fit(train_data[['Date', 'New_Cases']].rename(columns={'Date': 'ds', 'New_Cases': 'y'}))\n","\n","# Prophet Model for New Deaths\n","prophet_deaths = Prophet(yearly_seasonality=True, weekly_seasonality=True, changepoint_prior_scale=0.01)\n","prophet_deaths.fit(train_data[['Date', 'New_Deaths']].rename(columns={'Date': 'ds', 'New_Deaths': 'y'}))\n","\n","# Predictions\n","future_dates = prophet_cases.make_future_dataframe(periods=len(test_data))\n","prophet_cases_preds = prophet_cases.predict(future_dates)['yhat'][-len(test_data):]\n","prophet_deaths_preds = prophet_deaths.predict(future_dates)['yhat'][-len(test_data):]\n","\n","# Evaluate Prophet Results\n","prophet_rmse_cases = np.sqrt(mean_squared_error(test_data['New_Cases'], prophet_cases_preds))\n","prophet_rmse_deaths = np.sqrt(mean_squared_error(test_data['New_Deaths'], prophet_deaths_preds))\n","print(f\"Prophet New Cases RMSE: {prophet_rmse_cases}\")\n","print(f\"Prophet New Deaths RMSE: {prophet_rmse_deaths}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T09:52:20.933209Z","iopub.status.busy":"2024-10-13T09:52:20.932711Z","iopub.status.idle":"2024-10-13T10:00:53.878222Z","shell.execute_reply":"2024-10-13T10:00:53.877116Z","shell.execute_reply.started":"2024-10-13T09:52:20.933162Z"},"trusted":true},"outputs":[],"source":["from pmdarima import auto_arima\n","\n","# SARIMA for New Cases\n","sarima_cases = auto_arima(train_data['New_Cases'], seasonal=True, stepwise=True, trace=True)\n","sarima_cases_preds = sarima_cases.predict(n_periods=len(test_data))\n","\n","# SARIMA for New Deaths\n","sarima_deaths = auto_arima(train_data['New_Deaths'], seasonal=True, stepwise=True, trace=True)\n","sarima_deaths_preds = sarima_deaths.predict(n_periods=len(test_data))\n","\n","# Evaluate SARIMA Results\n","sarima_rmse_cases = np.sqrt(mean_squared_error(test_data['New_Cases'], sarima_cases_preds))\n","sarima_rmse_deaths = np.sqrt(mean_squared_error(test_data['New_Deaths'], sarima_deaths_preds))\n","print(f\"SARIMA New Cases RMSE: {sarima_rmse_cases}\")\n","print(f\"SARIMA New Deaths RMSE: {sarima_rmse_deaths}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:04:06.220731Z","iopub.status.busy":"2024-10-13T10:04:06.219783Z","iopub.status.idle":"2024-10-13T10:04:09.319814Z","shell.execute_reply":"2024-10-13T10:04:09.318587Z","shell.execute_reply.started":"2024-10-13T10:04:06.220681Z"},"trusted":true},"outputs":[],"source":["from xgboost import XGBRegressor\n","from sklearn.model_selection import GridSearchCV\n","\n","# Grid Search for XGBoost hyperparameter tuning\n","param_grid = {\n","    'n_estimators': [50, 100],\n","    'learning_rate': [0.01, 0.1],\n","    'max_depth': [3, 5]\n","}\n","xgb_cases = XGBRegressor()\n","grid_cases = GridSearchCV(xgb_cases, param_grid, cv=5, scoring='neg_mean_squared_error')\n","grid_cases.fit(train_data[['lag_1_cases', 'lag_2_cases']], train_data['New_Cases'])\n","\n","xgb_deaths = XGBRegressor()\n","grid_deaths = GridSearchCV(xgb_deaths, param_grid, cv=5, scoring='neg_mean_squared_error')\n","grid_deaths.fit(train_data[['lag_1_deaths', 'lag_2_deaths']], train_data['New_Deaths'])\n","\n","# Predictions for XGBoost\n","xgb_cases_preds = grid_cases.best_estimator_.predict(test_data[['lag_1_cases', 'lag_2_cases']])\n","xgb_deaths_preds = grid_deaths.best_estimator_.predict(test_data[['lag_1_deaths', 'lag_2_deaths']])\n","\n","# Evaluate XGBoost Results\n","xgb_rmse_cases = np.sqrt(mean_squared_error(test_data['New_Cases'], xgb_cases_preds))\n","xgb_rmse_deaths = np.sqrt(mean_squared_error(test_data['New_Deaths'], xgb_deaths_preds))\n","print(f\"XGBoost New Cases RMSE: {xgb_rmse_cases}\")\n","print(f\"XGBoost New Deaths RMSE: {xgb_rmse_deaths}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:32:20.148033Z","iopub.status.busy":"2024-10-13T10:32:20.147014Z","iopub.status.idle":"2024-10-13T10:32:20.159476Z","shell.execute_reply":"2024-10-13T10:32:20.158125Z","shell.execute_reply.started":"2024-10-13T10:32:20.147981Z"},"trusted":true},"outputs":[],"source":["# Weighting based on performance\n","weights = {\n","    'prophet': 0.4,  # Give Prophet more weight if it performs better in capturing trends\n","    'sarima': 0.3,   # SARIMA for seasonality and short-term patterns\n","    'xgb': 0.3       # XGBoost for short-term fluctuations\n","}\n","\n","# Final ensemble predictions (weighted average)\n","final_cases_preds = (weights['prophet'] * prophet_cases_preds) + (weights['sarima'] * sarima_cases_preds) + (weights['xgb'] * xgb_cases_preds)\n","final_deaths_preds = (weights['prophet'] * prophet_deaths_preds) + (weights['sarima'] * sarima_deaths_preds) + (weights['xgb'] * xgb_deaths_preds)\n","\n","# Evaluate Ensemble\n","rmse_cases_ensemble = np.sqrt(mean_squared_error(test_data['New_Cases'], final_cases_preds))\n","rmse_deaths_ensemble = np.sqrt(mean_squared_error(test_data['New_Deaths'], final_deaths_preds))\n","\n","print(f\"Ensemble New Cases RMSE: {rmse_cases_ensemble}\")\n","print(f\"Ensemble New Deaths RMSE: {rmse_deaths_ensemble}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:32:31.289312Z","iopub.status.busy":"2024-10-13T10:32:31.288866Z","iopub.status.idle":"2024-10-13T10:32:32.112318Z","shell.execute_reply":"2024-10-13T10:32:32.111130Z","shell.execute_reply.started":"2024-10-13T10:32:31.289267Z"},"trusted":true},"outputs":[],"source":["# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(test_data['Date'], test_data['New_Cases'], label='Actual New Cases')\n","plt.plot(test_data['Date'], final_cases_preds, label='Ensemble Predicted New Cases')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (Ensemble)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(test_data['Date'], test_data['New_Deaths'], label='Actual New Deaths')\n","plt.plot(test_data['Date'], final_deaths_preds, label='Ensemble Predicted New Deaths')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (Ensemble)')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Deep learning way**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:32:40.456280Z","iopub.status.busy":"2024-10-13T10:32:40.455847Z","iopub.status.idle":"2024-10-13T10:32:52.173348Z","shell.execute_reply":"2024-10-13T10:32:52.171878Z","shell.execute_reply.started":"2024-10-13T10:32:40.456236Z"},"trusted":true},"outputs":[],"source":["!pip install tensorflow scikit-learn matplotlib pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:33:33.861025Z","iopub.status.busy":"2024-10-13T10:33:33.860022Z","iopub.status.idle":"2024-10-13T10:33:33.896215Z","shell.execute_reply":"2024-10-13T10:33:33.895138Z","shell.execute_reply.started":"2024-10-13T10:33:33.860965Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","\n","# Remove outliers based on quantiles\n","q_low_cases = data['New_Cases'].quantile(0.01)\n","q_high_cases = data['New_Cases'].quantile(0.99)\n","q_low_deaths = data['New_Deaths'].quantile(0.01)\n","q_high_deaths = data['New_Deaths'].quantile(0.99)\n","\n","data = data[(data['New_Cases'] > q_low_cases) & (data['New_Cases'] < q_high_cases)]\n","data = data[(data['New_Deaths'] > q_low_deaths) & (data['New_Deaths'] < q_high_deaths)]\n","\n","# Prepare features and targets for LSTM\n","data = data[['Date', 'New_Cases', 'New_Deaths']]\n","data.set_index('Date', inplace=True)\n","\n","# Scale the data\n","scaler_cases = MinMaxScaler(feature_range=(0, 1))\n","scaled_cases = scaler_cases.fit_transform(data[['New_Cases']])\n","\n","scaler_deaths = MinMaxScaler(feature_range=(0, 1))\n","scaled_deaths = scaler_deaths.fit_transform(data[['New_Deaths']])\n","\n","# Convert data to time-series format for LSTM\n","def create_lstm_dataset(dataset, time_step=1):\n","    X, Y = [], []\n","    for i in range(len(dataset) - time_step - 1):\n","        X.append(dataset[i:(i + time_step), 0])\n","        Y.append(dataset[i + time_step, 0])\n","    return np.array(X), np.array(Y)\n","\n","# Create datasets for cases and deaths\n","time_step = 7  # Considering 7 days of history for predictions\n","X_cases, Y_cases = create_lstm_dataset(scaled_cases, time_step)\n","X_deaths, Y_deaths = create_lstm_dataset(scaled_deaths, time_step)\n","\n","# Reshape input to be [samples, time steps, features] which is required for LSTM\n","X_cases = X_cases.reshape(X_cases.shape[0], X_cases.shape[1], 1)\n","X_deaths = X_deaths.reshape(X_deaths.shape[0], X_deaths.shape[1], 1)\n","\n","# Split into train and test\n","split_ratio = 0.8\n","split_idx_cases = int(len(X_cases) * split_ratio)\n","split_idx_deaths = int(len(X_deaths) * split_ratio)\n","\n","X_train_cases, X_test_cases = X_cases[:split_idx_cases], X_cases[split_idx_cases:]\n","Y_train_cases, Y_test_cases = Y_cases[:split_idx_cases], Y_cases[split_idx_cases:]\n","\n","X_train_deaths, X_test_deaths = X_deaths[:split_idx_deaths], X_deaths[split_idx_deaths:]\n","Y_train_deaths, Y_test_deaths = Y_deaths[:split_idx_deaths], Y_deaths[split_idx_deaths:]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:33:37.572498Z","iopub.status.busy":"2024-10-13T10:33:37.572031Z","iopub.status.idle":"2024-10-13T10:33:53.179022Z","shell.execute_reply":"2024-10-13T10:33:53.177790Z","shell.execute_reply.started":"2024-10-13T10:33:37.572458Z"},"trusted":true},"outputs":[],"source":["#gpt# vai er ta run hoccchilo na#\n","# Import necessary libraries\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dropout, Dense\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","\n","# Example dataset (replace with your actual dataset)\n","cases_data = np.random.rand(1000, 1)  # Replace with actual data\n","\n","# Scale the data\n","scaler_cases = MinMaxScaler(feature_range=(0, 1))\n","scaled_cases_data = scaler_cases.fit_transform(cases_data)\n","\n","# Define the time step\n","time_step = 10  # The number of time steps for LSTM input\n","X, Y = [], []\n","\n","# Create the sequences for LSTM\n","for i in range(time_step, len(scaled_cases_data)):\n","    X.append(scaled_cases_data[i-time_step:i, 0])  # Create input sequences\n","    Y.append(scaled_cases_data[i, 0])  # Create output sequences (next step)\n","\n","# Convert to numpy arrays\n","X, Y = np.array(X), np.array(Y)\n","\n","# Reshape X for LSTM input (samples, time steps, features)\n","X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape to (samples, time steps, 1 feature)\n","\n","# Split the data into training and testing sets\n","X_train_cases, X_test_cases, Y_train_cases, Y_test_cases = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","# LSTM model for New Cases\n","model_cases = Sequential()\n","model_cases.add(LSTM(100, return_sequences=True, input_shape=(time_step, 1)))\n","model_cases.add(Dropout(0.2))\n","model_cases.add(LSTM(50, return_sequences=False))\n","model_cases.add(Dropout(0.2))\n","model_cases.add(Dense(1))\n","\n","# Compile the model\n","model_cases.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","model_cases.fit(X_train_cases, Y_train_cases, epochs=20, batch_size=32, verbose=1)\n","\n","# Predict on test data\n","pred_cases = model_cases.predict(X_test_cases)\n","\n","# Reshape Y_test_cases before inverse transformation\n","Y_test_cases = Y_test_cases.reshape(-1, 1)\n","\n","# Inverse transform the predictions and actual values to original scale\n","pred_cases = scaler_cases.inverse_transform(pred_cases)\n","Y_test_cases = scaler_cases.inverse_transform(Y_test_cases)\n","\n","# Calculate RMSE for New Cases\n","rmse_cases = np.sqrt(mean_squared_error(Y_test_cases, pred_cases))\n","print(f\"LSTM New Cases RMSE: {rmse_cases}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:34:13.817685Z","iopub.status.busy":"2024-10-13T10:34:13.817167Z","iopub.status.idle":"2024-10-13T10:34:29.942259Z","shell.execute_reply":"2024-10-13T10:34:29.941026Z","shell.execute_reply.started":"2024-10-13T10:34:13.817638Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","\n","# Load the dataset (update the file path to your dataset)\n","# Ensure that the dataset has a column named 'New_Deaths'\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","\n","# Check if the 'New_Deaths' column exists\n","if 'New_Deaths' not in data.columns:\n","    raise ValueError(\"'New_Deaths' column is missing from the dataset!\")\n","\n","# Define the column from your data representing New Deaths\n","new_deaths = data['New_Deaths'].values\n","\n","# Scaling the data\n","scaler_deaths = MinMaxScaler(feature_range=(0, 1))\n","new_deaths_scaled = scaler_deaths.fit_transform(new_deaths.reshape(-1, 1))\n","\n","# Create a function to prepare your dataset for LSTM\n","def create_dataset(dataset, time_step=1):\n","    X, Y = [], []\n","    for i in range(len(dataset) - time_step - 1):\n","        X.append(dataset[i:(i + time_step), 0])\n","        Y.append(dataset[i + time_step, 0])\n","    return np.array(X), np.array(Y)\n","\n","# Define the time_step for LSTM\n","time_step = 10\n","X, Y = create_dataset(new_deaths_scaled, time_step)\n","\n","# Reshape the input to be [samples, time steps, features] as required by LSTM\n","X = X.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Split the dataset into training and testing data\n","train_size = int(len(X) * 0.8)\n","test_size = len(X) - train_size\n","X_train_deaths, X_test_deaths = X[0:train_size], X[train_size:len(X)]\n","Y_train_deaths, Y_test_deaths = Y[0:train_size], Y[train_size:len(Y)]\n","\n","# Define the LSTM model\n","model_deaths = Sequential()\n","model_deaths.add(LSTM(100, return_sequences=True, input_shape=(time_step, 1)))\n","model_deaths.add(Dropout(0.2))\n","model_deaths.add(LSTM(50, return_sequences=False))\n","model_deaths.add(Dropout(0.2))\n","model_deaths.add(Dense(1))\n","\n","# Compile the model\n","model_deaths.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","model_deaths.fit(X_train_deaths, Y_train_deaths, epochs=20, batch_size=32, verbose=1)\n","\n","# Predict on test data\n","pred_deaths = model_deaths.predict(X_test_deaths)\n","\n","# Inverse transform the predictions to original scale\n","pred_deaths = scaler_deaths.inverse_transform(pred_deaths)\n","Y_test_deaths = scaler_deaths.inverse_transform([Y_test_deaths])\n","\n","# Calculate RMSE for New Deaths\n","rmse_deaths = np.sqrt(mean_squared_error(Y_test_deaths[0], pred_deaths[:, 0]))\n","print(f\"LSTM New Deaths RMSE: {rmse_deaths}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:18:43.932338Z","iopub.status.busy":"2024-10-13T11:18:43.931335Z","iopub.status.idle":"2024-10-13T11:18:44.665254Z","shell.execute_reply":"2024-10-13T11:18:44.664259Z","shell.execute_reply.started":"2024-10-13T11:18:43.932292Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# ============================\n","# Define Your Actual Data Here\n","# ============================\n","\n","# Example: Replace the following synthetic data with your actual test data and predictions.\n","\n","# Number of time steps in the test set\n","num_time_steps_cases = 50\n","num_time_steps_deaths = 50\n","\n","# Synthetic actual new cases data (Replace with your actual Y_test_cases)\n","# Shape: (num_time_steps_cases, 1)\n","Y_test_cases = np.random.randint(50, 100, size=(num_time_steps_cases, 1))\n","\n","# Synthetic predicted new cases data (Replace with your actual pred_cases)\n","# Shape: (num_time_steps_cases, 1)\n","pred_cases = Y_test_cases + np.random.randint(-10, 10, size=(num_time_steps_cases, 1))\n","\n","# Synthetic actual new deaths data (Replace with your actual Y_test_deaths)\n","# Shape: (num_time_steps_deaths, 1)\n","Y_test_deaths = np.random.randint(5, 20, size=(num_time_steps_deaths, 1))\n","\n","# Synthetic predicted new deaths data (Replace with your actual pred_deaths)\n","# Shape: (num_time_steps_deaths, 1)\n","pred_deaths = Y_test_deaths + np.random.randint(-3, 3, size=(num_time_steps_deaths, 1))\n","\n","# ====================================\n","# Define Time Indices Based on Data\n","# ====================================\n","\n","# If you have actual dates, you can use them instead of a range of integers.\n","# For example, if you have a list of dates called `dates`, you can use:\n","# time_indices_cases = dates[-num_time_steps_cases:]\n","# time_indices_deaths = dates[-num_time_steps_deaths:]\n","\n","# Using a range of integers as time indices\n","time_indices_cases = np.arange(len(Y_test_cases))\n","time_indices_deaths = np.arange(len(Y_test_deaths))\n","\n","# ========================\n","# Plotting the Predictions\n","# ========================\n","\n","# Plot predictions for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(time_indices_cases, Y_test_cases[:, 0], label='Actual New Cases', marker='o')\n","plt.plot(time_indices_cases, pred_cases[:, 0], label='Predicted New Cases', marker='x')\n","plt.title('Actual vs Predicted New Cases (LSTM)')\n","plt.xlabel('Time')\n","plt.ylabel('New Cases')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Plot predictions for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(time_indices_deaths, Y_test_deaths[:, 0], label='Actual New Deaths', marker='o')\n","plt.plot(time_indices_deaths, pred_deaths[:, 0], label='Predicted New Deaths', marker='x')\n","plt.title('Actual vs Predicted New Deaths (LSTM)')\n","plt.xlabel('Time')\n","plt.ylabel('New Deaths')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Hybrid prpht, Xgiboost Sarima,Lstm with prophet**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:46:32.011579Z","iopub.status.busy":"2024-10-13T10:46:32.011166Z","iopub.status.idle":"2024-10-13T10:46:34.599443Z","shell.execute_reply":"2024-10-13T10:46:34.598370Z","shell.execute_reply.started":"2024-10-13T10:46:32.011540Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","\n","# Load the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","data.set_index('Date', inplace=True)\n","\n","# Decompose the time series for New Cases and New Deaths\n","result_cases = seasonal_decompose(data['New_Cases'], model='additive', period=7)\n","result_deaths = seasonal_decompose(data['New_Deaths'], model='additive', period=7)\n","\n","# Plot the decomposition\n","result_cases.plot()\n","plt.title('Decomposition of New Cases')\n","plt.show()\n","\n","result_deaths.plot()\n","plt.title('Decomposition of New Deaths')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:46:40.783520Z","iopub.status.busy":"2024-10-13T10:46:40.783123Z","iopub.status.idle":"2024-10-13T10:46:40.803175Z","shell.execute_reply":"2024-10-13T10:46:40.802004Z","shell.execute_reply.started":"2024-10-13T10:46:40.783482Z"},"trusted":true},"outputs":[],"source":["# Add time-based features\n","data['day_of_week'] = data.index.dayofweek\n","data['month'] = data.index.month\n","data['week_of_year'] = data.index.isocalendar().week\n","\n","# Add rolling statistics for smoothing\n","data['rolling_mean_cases'] = data['New_Cases'].rolling(window=7).mean().fillna(method='bfill')\n","data['rolling_std_cases'] = data['New_Cases'].rolling(window=7).std().fillna(method='bfill')\n","data['rolling_mean_deaths'] = data['New_Deaths'].rolling(window=7).mean().fillna(method='bfill')\n","data['rolling_std_deaths'] = data['New_Deaths'].rolling(window=7).std().fillna(method='bfill')\n","\n","# Differencing to achieve stationarity\n","data['diff_cases'] = data['New_Cases'].diff().fillna(0)\n","data['diff_deaths'] = data['New_Deaths'].diff().fillna(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:46:47.240220Z","iopub.status.busy":"2024-10-13T10:46:47.239754Z","iopub.status.idle":"2024-10-13T10:46:58.191935Z","shell.execute_reply":"2024-10-13T10:46:58.190369Z","shell.execute_reply.started":"2024-10-13T10:46:47.240169Z"},"trusted":true},"outputs":[],"source":["!pip install pmdarima\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:47:10.287368Z","iopub.status.busy":"2024-10-13T10:47:10.286347Z","iopub.status.idle":"2024-10-13T10:48:56.704297Z","shell.execute_reply":"2024-10-13T10:48:56.703341Z","shell.execute_reply.started":"2024-10-13T10:47:10.287318Z"},"trusted":true},"outputs":[],"source":["from pmdarima import auto_arima\n","\n","# SARIMA for New Cases\n","sarima_cases = auto_arima(data['New_Cases'], seasonal=True, stepwise=True)\n","sarima_cases_preds = sarima_cases.predict(n_periods=len(data))\n","\n","# SARIMA for New Deaths\n","sarima_deaths = auto_arima(data['New_Deaths'], seasonal=True, stepwise=True)\n","sarima_deaths_preds = sarima_deaths.predict(n_periods=len(data))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:49:52.189608Z","iopub.status.busy":"2024-10-13T10:49:52.189185Z","iopub.status.idle":"2024-10-13T10:50:18.491153Z","shell.execute_reply":"2024-10-13T10:50:18.490131Z","shell.execute_reply.started":"2024-10-13T10:49:52.189567Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Scaling the data\n","scaler_cases = MinMaxScaler(feature_range=(0, 1))\n","scaled_cases = scaler_cases.fit_transform(data[['New_Cases']])\n","\n","scaler_deaths = MinMaxScaler(feature_range=(0, 1))\n","scaled_deaths = scaler_deaths.fit_transform(data[['New_Deaths']])\n","\n","# Function to create dataset for LSTM\n","def create_lstm_dataset(dataset, time_step=1):\n","    X, Y = [], []\n","    for i in range(len(dataset) - time_step - 1):\n","        X.append(dataset[i:(i + time_step), 0])\n","        Y.append(dataset[i + time_step, 0])\n","    return np.array(X), np.array(Y)\n","\n","time_step = 7  # History of 7 days\n","X_cases, Y_cases = create_lstm_dataset(scaled_cases, time_step)\n","X_deaths, Y_deaths = create_lstm_dataset(scaled_deaths, time_step)\n","\n","# Reshape input for LSTM [samples, time steps, features]\n","X_cases = X_cases.reshape(X_cases.shape[0], X_cases.shape[1], 1)\n","X_deaths = X_deaths.reshape(X_deaths.shape[0], X_deaths.shape[1], 1)\n","\n","# LSTM Model for New Cases\n","model_cases = Sequential()\n","model_cases.add(LSTM(100, return_sequences=True, input_shape=(time_step, 1)))\n","model_cases.add(Dropout(0.2))\n","model_cases.add(LSTM(50, return_sequences=False))\n","model_cases.add(Dropout(0.2))\n","model_cases.add(Dense(1))\n","\n","model_cases.compile(optimizer='adam', loss='mean_squared_error')\n","model_cases.fit(X_cases, Y_cases, epochs=20, batch_size=32, verbose=1)\n","\n","# LSTM Model for New Deaths\n","model_deaths = Sequential()\n","model_deaths.add(LSTM(100, return_sequences=True, input_shape=(time_step, 1)))\n","model_deaths.add(Dropout(0.2))\n","model_deaths.add(LSTM(50, return_sequences=False))\n","model_deaths.add(Dropout(0.2))\n","model_deaths.add(Dense(1))\n","\n","model_deaths.compile(optimizer='adam', loss='mean_squared_error')\n","model_deaths.fit(X_deaths, Y_deaths, epochs=20, batch_size=32, verbose=1)\n","\n","# Predictions\n","pred_cases = model_cases.predict(X_cases)\n","pred_deaths = model_deaths.predict(X_deaths)\n","\n","# Inverse scaling\n","pred_cases = scaler_cases.inverse_transform(pred_cases)\n","pred_deaths = scaler_deaths.inverse_transform(pred_deaths)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:51:44.693552Z","iopub.status.busy":"2024-10-13T10:51:44.693135Z","iopub.status.idle":"2024-10-13T10:51:44.823904Z","shell.execute_reply":"2024-10-13T10:51:44.823023Z","shell.execute_reply.started":"2024-10-13T10:51:44.693512Z"},"trusted":true},"outputs":[],"source":["from xgboost import XGBRegressor\n","\n","# XGBoost for New Cases\n","xgb_cases = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)\n","xgb_cases.fit(data[['day_of_week', 'month', 'rolling_mean_cases', 'rolling_std_cases']], data['New_Cases'])\n","xgb_cases_preds = xgb_cases.predict(data[['day_of_week', 'month', 'rolling_mean_cases', 'rolling_std_cases']])\n","\n","# XGBoost for New Deaths\n","xgb_deaths = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5)\n","xgb_deaths.fit(data[['day_of_week', 'month', 'rolling_mean_deaths', 'rolling_std_deaths']], data['New_Deaths'])\n","xgb_deaths_preds = xgb_deaths.predict(data[['day_of_week', 'month', 'rolling_mean_deaths', 'rolling_std_deaths']])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:51:51.975164Z","iopub.status.busy":"2024-10-13T10:51:51.974324Z","iopub.status.idle":"2024-10-13T10:51:51.984209Z","shell.execute_reply":"2024-10-13T10:51:51.982884Z","shell.execute_reply.started":"2024-10-13T10:51:51.975112Z"},"trusted":true},"outputs":[],"source":["# Find the minimum length of predictions\n","min_length_cases = min(len(sarima_cases_preds), len(pred_cases), len(xgb_cases_preds))\n","min_length_deaths = min(len(sarima_deaths_preds), len(pred_deaths), len(xgb_deaths_preds))\n","\n","# Truncate predictions to the same length\n","sarima_cases_preds = sarima_cases_preds[:min_length_cases]\n","pred_cases = pred_cases[:min_length_cases]\n","xgb_cases_preds = xgb_cases_preds[:min_length_cases]\n","\n","sarima_deaths_preds = sarima_deaths_preds[:min_length_deaths]\n","pred_deaths = pred_deaths[:min_length_deaths]\n","xgb_deaths_preds = xgb_deaths_preds[:min_length_deaths]\n","\n","# Ensemble with simple averaging (after truncating)\n","final_cases_preds = (0.3 * sarima_cases_preds) + (0.4 * pred_cases.flatten()) + (0.3 * xgb_cases_preds)\n","final_deaths_preds = (0.3 * sarima_deaths_preds) + (0.4 * pred_deaths.flatten()) + (0.3 * xgb_deaths_preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:54:54.582805Z","iopub.status.busy":"2024-10-13T10:54:54.581944Z","iopub.status.idle":"2024-10-13T10:54:54.592271Z","shell.execute_reply":"2024-10-13T10:54:54.591075Z","shell.execute_reply.started":"2024-10-13T10:54:54.582753Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics import mean_squared_error  # Import the missing function\n","\n","# Truncate the actual values for cases and deaths\n","actual_cases = data['New_Cases'][-min_length_cases:]\n","actual_deaths = data['New_Deaths'][-min_length_deaths:]\n","\n","# Evaluation metrics\n","rmse_cases = np.sqrt(mean_squared_error(actual_cases, final_cases_preds))\n","rmse_deaths = np.sqrt(mean_squared_error(actual_deaths, final_deaths_preds))\n","\n","print(f\"Final RMSE New Cases: {rmse_cases}\")\n","print(f\"Final RMSE New Deaths: {rmse_deaths}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T10:55:13.506253Z","iopub.status.busy":"2024-10-13T10:55:13.505839Z","iopub.status.idle":"2024-10-13T10:55:14.066179Z","shell.execute_reply":"2024-10-13T10:55:14.065151Z","shell.execute_reply.started":"2024-10-13T10:55:13.506217Z"},"trusted":true},"outputs":[],"source":["# Plot predictions for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(actual_cases.index, actual_cases, label='Actual New Cases')\n","plt.plot(actual_cases.index, final_cases_preds, label='Predicted New Cases (Hybrid)')\n","plt.title('Actual vs Predicted New Cases (Hybrid Model)')\n","plt.legend()\n","plt.show()\n","\n","# Plot predictions for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(actual_deaths.index, actual_deaths, label='Actual New Deaths')\n","plt.plot(actual_deaths.index, final_deaths_preds, label='Predicted New Deaths (Hybrid Model)')\n","plt.title('Actual vs Predicted New Deaths (Hybrid Model)')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**LightGBM**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:00:57.895216Z","iopub.status.busy":"2024-10-13T11:00:57.894377Z","iopub.status.idle":"2024-10-13T11:00:57.920963Z","shell.execute_reply":"2024-10-13T11:00:57.919918Z","shell.execute_reply.started":"2024-10-13T11:00:57.895173Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.preprocessing import MinMaxScaler\n","import matplotlib.pyplot as plt\n","\n","# Load and prepare the dataset\n","data = pd.read_csv('/kaggle/input/final-dengu-journal-work/Dengue_final_as_MPOX.csv')\n","data['Date'] = pd.to_datetime(data['Date'], format='%m/%d/%Y')\n","data.set_index('Date', inplace=True)\n","\n","# Feature engineering\n","data['day_of_week'] = data.index.dayofweek\n","data['month'] = data.index.month\n","data['week_of_year'] = data.index.isocalendar().week\n","data['lag_1_cases'] = data['New_Cases'].shift(1).fillna(0)\n","data['lag_2_cases'] = data['New_Cases'].shift(2).fillna(0)\n","data['lag_1_deaths'] = data['New_Deaths'].shift(1).fillna(0)\n","data['lag_2_deaths'] = data['New_Deaths'].shift(2).fillna(0)\n","\n","# Train-test split using time-based cross-validation\n","X = data[['day_of_week', 'month', 'week_of_year', 'lag_1_cases', 'lag_2_cases', 'lag_1_deaths', 'lag_2_deaths']]\n","y_cases = data['New_Cases']\n","y_deaths = data['New_Deaths']\n","\n","# TimeSeriesSplit for time-based cross-validation\n","tscv = TimeSeriesSplit(n_splits=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:01:01.161180Z","iopub.status.busy":"2024-10-13T11:01:01.160395Z","iopub.status.idle":"2024-10-13T11:01:12.878443Z","shell.execute_reply":"2024-10-13T11:01:12.877367Z","shell.execute_reply.started":"2024-10-13T11:01:01.161136Z"},"trusted":true},"outputs":[],"source":["import lightgbm as lgb\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","\n","# Define the LightGBM model for cases and deaths\n","lgb_cases = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression', n_jobs=-1)\n","lgb_deaths = lgb.LGBMRegressor(boosting_type='gbdt', objective='regression', n_jobs=-1)\n","\n","# Define parameter grid for LightGBM\n","param_grid = {\n","    'n_estimators': [50, 100, 150],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'max_depth': [3, 5, 7]\n","}\n","\n","# Perform grid search with cross-validation\n","grid_cases = GridSearchCV(lgb_cases, param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n","grid_cases.fit(X, y_cases)\n","\n","grid_deaths = GridSearchCV(lgb_deaths, param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n","grid_deaths.fit(X, y_deaths)\n","\n","# Get the best models\n","best_lgb_cases = grid_cases.best_estimator_\n","best_lgb_deaths = grid_deaths.best_estimator_"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:02:29.482052Z","iopub.status.busy":"2024-10-13T11:02:29.480326Z","iopub.status.idle":"2024-10-13T11:02:29.503934Z","shell.execute_reply":"2024-10-13T11:02:29.502835Z","shell.execute_reply.started":"2024-10-13T11:02:29.482003Z"},"trusted":true},"outputs":[],"source":["# Make predictions on the test set\n","pred_cases = best_lgb_cases.predict(X)\n","pred_deaths = best_lgb_deaths.predict(X)\n","\n","# Evaluate model performance\n","rmse_cases = np.sqrt(mean_squared_error(y_cases, pred_cases))\n","rmse_deaths = np.sqrt(mean_squared_error(y_deaths, pred_deaths))\n","mae_cases = mean_absolute_error(y_cases, pred_cases)\n","mae_deaths = mean_absolute_error(y_deaths, pred_deaths)\n","r2_cases = r2_score(y_cases, pred_cases)\n","r2_deaths = r2_score(y_deaths, pred_deaths)\n","\n","# Print results\n","print(f\"LightGBM New Cases - RMSE: {rmse_cases}, MAE: {mae_cases}, R-squared: {r2_cases}\")\n","print(f\"LightGBM New Deaths - RMSE: {rmse_deaths}, MAE: {mae_deaths}, R-squared: {r2_deaths}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:02:41.934577Z","iopub.status.busy":"2024-10-13T11:02:41.933595Z","iopub.status.idle":"2024-10-13T11:02:42.710959Z","shell.execute_reply":"2024-10-13T11:02:42.709887Z","shell.execute_reply.started":"2024-10-13T11:02:41.934519Z"},"trusted":true},"outputs":[],"source":["# Plotting the forecast and actuals for New Cases\n","plt.figure(figsize=(10, 6))\n","plt.plot(data.index, y_cases, label='Actual New Cases')\n","plt.plot(data.index, pred_cases, label='Predicted New Cases (LightGBM)')\n","plt.xlabel('Date')\n","plt.ylabel('New Cases')\n","plt.title('Actual vs Predicted New Cases (LightGBM)')\n","plt.legend()\n","plt.show()\n","\n","# Plotting the forecast and actuals for New Deaths\n","plt.figure(figsize=(10, 6))\n","plt.plot(data.index, y_deaths, label='Actual New Deaths')\n","plt.plot(data.index, pred_deaths, label='Predicted New Deaths (LightGBM)')\n","plt.xlabel('Date')\n","plt.ylabel('New Deaths')\n","plt.title('Actual vs Predicted New Deaths (LightGBM)')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:02:47.374861Z","iopub.status.busy":"2024-10-13T11:02:47.374413Z","iopub.status.idle":"2024-10-13T11:02:56.835780Z","shell.execute_reply":"2024-10-13T11:02:56.834683Z","shell.execute_reply.started":"2024-10-13T11:02:47.374818Z"},"trusted":true},"outputs":[],"source":["import optuna\n","from sklearn.model_selection import cross_val_score  # Import cross_val_score for cross-validation\n","\n","# Objective function for Bayesian optimization with LightGBM\n","def objective(trial):\n","    params = {\n","        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n","        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n","        'max_depth': trial.suggest_int('max_depth', 3, 10),\n","        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n","        'min_child_samples': trial.suggest_int('min_child_samples', 5, 30)\n","    }\n","    \n","    # Initialize the model with suggested parameters\n","    model = lgb.LGBMRegressor(**params)\n","    \n","    # Perform time-series cross-validation and calculate the score\n","    score = -np.mean(cross_val_score(model, X, y_cases, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1))\n","    \n","    return score\n","\n","# Run the optimization\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=50)\n","\n","# Get the best parameters\n","best_params = study.best_params\n","print(\"Best parameters:\", best_params)\n","\n","# Train the best model with optimized hyperparameters\n","best_lgb_cases = lgb.LGBMRegressor(**best_params)\n","best_lgb_cases.fit(X, y_cases)\n","\n","best_lgb_deaths = lgb.LGBMRegressor(**best_params)\n","best_lgb_deaths.fit(X, y_deaths)\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:05:51.425039Z","iopub.status.busy":"2024-10-13T11:05:51.423941Z","iopub.status.idle":"2024-10-13T11:06:04.437250Z","shell.execute_reply":"2024-10-13T11:06:04.436142Z","shell.execute_reply.started":"2024-10-13T11:05:51.424991Z"},"trusted":true},"outputs":[],"source":["!pip install pytorch-lightning pytorch-forecasting\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:06:18.159716Z","iopub.status.busy":"2024-10-13T11:06:18.158461Z","iopub.status.idle":"2024-10-13T11:06:29.562319Z","shell.execute_reply":"2024-10-13T11:06:29.561138Z","shell.execute_reply.started":"2024-10-13T11:06:18.159667Z"},"trusted":true},"outputs":[],"source":["!pip install pytorch-forecasting pytorch-lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:06:38.574679Z","iopub.status.busy":"2024-10-13T11:06:38.573713Z","iopub.status.idle":"2024-10-13T11:08:14.345594Z","shell.execute_reply":"2024-10-13T11:08:14.344131Z","shell.execute_reply.started":"2024-10-13T11:06:38.574628Z"},"trusted":true},"outputs":[],"source":["!pip install pytorch-forecasting pytorch-lightning Metric"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:13:01.077632Z","iopub.status.busy":"2024-10-13T11:13:01.077226Z","iopub.status.idle":"2024-10-13T11:13:01.140278Z","shell.execute_reply":"2024-10-13T11:13:01.138863Z","shell.execute_reply.started":"2024-10-13T11:13:01.077590Z"},"trusted":true},"outputs":[],"source":["import pytorch_lightning as pl  # Import the PyTorch Lightning module\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","\n","# Now we define the LightningModule explicitly\n","class TFTLightningModel(pl.LightningModule):\n","    def __init__(self, tft):\n","        super().__init__()\n","        self.model = tft\n","        self.save_hyperparameters(ignore=['tft', 'loss'])  # Ignore 'tft' and 'loss' during checkpointing\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_pred = self.model(x)  # Forward pass\n","        if isinstance(y_pred, tuple):\n","            y_pred = y_pred[0]  # If output is a tuple, extract the predictions\n","        loss = self.model.loss(y_pred, y)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_pred = self.model(x)  # Forward pass\n","        if isinstance(y_pred, tuple):\n","            y_pred = y_pred[0]  # If output is a tuple, extract the predictions\n","        loss = self.model.loss(y_pred, y)\n","        self.log('val_loss', loss, prog_bar=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.model.hparams.learning_rate)\n","\n","# Initialize the LightningModule with the TemporalFusionTransformer\n","tft_lightning = TFTLightningModel(tft)\n","\n","# Update Trainer configuration\n","trainer = pl.Trainer(\n","    max_epochs=30,\n","    devices=1,\n","    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n","    log_every_n_steps=10,\n","    callbacks=[pl.callbacks.ModelCheckpoint(save_weights_only=True)]\n",")\n","\n","# Train the model\n","trainer.fit(tft_lightning, train_dataloader, val_dataloader)\n","\n","# Make predictions\n","predictions = tft.predict(val_dataloader, mode=\"prediction\")\n","\n","# Check the shape of predictions\n","print(f\"Shape of predictions: {predictions.shape}\")\n","\n","# Extract the relevant part of predictions\n","if predictions.ndim > 2:\n","    predictions = predictions[:, :, 0]\n","\n","# Reshape predictions to match scaler input\n","predictions = predictions[:, 0].reshape(-1, 1)\n","\n","# Inverse scaling to get original values\n","predictions_inverse = scaler.inverse_transform(predictions)\n","\n","# Perform inverse transform for validation targets\n","val_targets = torch.cat([y for x, y in iter(val_dataloader)], dim=0).cpu().numpy()\n","val_targets_inverse = scaler.inverse_transform(val_targets.reshape(-1, 1))\n","\n","# Calculate RMSE\n","rmse = mean_squared_error(val_targets_inverse, predictions_inverse, squared=False)\n","print(f\"RMSE: {rmse}\")\n","\n","# Plot results\n","plt.plot(val_targets_inverse, label=\"Actual\")\n","plt.plot(predictions_inverse, label=\"Predicted\")\n","plt.legend()\n","plt.title(\"Actual vs Predicted Cases\")\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pytorch_lightning as pl\n","from pytorch_forecasting import TemporalFusionTransformer, Baseline\n","from pytorch_forecasting.metrics import QuantileLoss\n","\n","# Split data into train and validation sets\n","train_dataloader = tft_dataset.to_dataloader(train=True, batch_size=64, num_workers=0)\n","val_dataloader = tft_dataset.to_dataloader(train=False, batch_size=64, num_workers=0)\n","\n","# Define the TFT model\n","tft = TemporalFusionTransformer.from_dataset(\n","    tft_dataset,\n","    learning_rate=0.03,\n","    hidden_size=32,  # model size parameter\n","    attention_head_size=4,\n","    dropout=0.1,\n","    hidden_continuous_size=16,\n","    loss=QuantileLoss(),\n","    log_interval=10,\n","    reduce_on_plateau_patience=4,\n",")\n","\n","# Train the model\n","trainer = pl.Trainer(\n","    max_epochs=30,\n","    gpus=0,  # set to 1 if using GPU\n",")\n","trainer.fit(tft, train_dataloader, val_dataloader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-13T11:09:17.110937Z","iopub.status.busy":"2024-10-13T11:09:17.109966Z","iopub.status.idle":"2024-10-13T11:09:17.150193Z","shell.execute_reply":"2024-10-13T11:09:17.148612Z","shell.execute_reply.started":"2024-10-13T11:09:17.110892Z"},"trusted":true},"outputs":[],"source":["# Predict on validation set\n","predictions = tft.predict(val_dataloader, mode=\"prediction\")\n","\n","# Inverse scaling to get original values\n","predictions = scaler.inverse_transform(predictions)\n","actuals = scaler.inverse_transform(val_dataloader.dataset.target_scaled)\n","\n","# Calculate RMSE\n","from sklearn.metrics import mean_squared_error\n","rmse = np.sqrt(mean_squared_error(actuals, predictions))\n","print(f\"TFT RMSE: {rmse}\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5865230,"sourceId":9612039,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
